<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>probes - prometheus</title>
    <link rel="self" type="application/atom+xml" href="https://clux.dev/tags/prometheus/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://clux.dev"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2024-09-07T00:00:00+00:00</updated>
    <id>https://clux.dev/tags/prometheus/atom.xml</id>
    <entry xml:lang="en">
        <title>Running a Minimal Prometheus</title>
        <published>2024-09-07T00:00:00+00:00</published>
        <updated>2024-09-07T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://clux.dev/post/2024-09-07-prometheus-minified/"/>
        <id>https://clux.dev/post/2024-09-07-prometheus-minified/</id>
        
        <content type="html" xml:base="https://clux.dev/post/2024-09-07-prometheus-minified/">&lt;p&gt;Prometheus does not need to be hugely complicated, nor a massive resource hog, provided you follow some principles.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;background&quot;&gt;Background&lt;&#x2F;h2&gt;
&lt;p&gt;My last &lt;a href=&quot;&#x2F;tags&#x2F;prometheus&#x2F;&quot;&gt;#prometheus&lt;&#x2F;a&gt; posts have been exclusively about large scale production setups, and the difficulties this pulls in.&lt;&#x2F;p&gt;
&lt;p&gt;I would like to argue that these difficulties are often self-imposed, and a combination result of inadequate &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;prometheus.io&#x2F;docs&#x2F;practices&#x2F;instrumentation&#x2F;#do-not-overuse-labels&quot;&gt;cardinality control&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Induced_demand&quot;&gt;induced demand&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;ðŸ‘º: You should be able to run a prometheus on your handful-of-machine-sized homelab with &amp;lt;10k time series active, using less than 512Mi memory, and 10m CPU.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;details&gt;&lt;summary style=&quot;cursor:pointer;color:#0af&quot;&gt;&lt;b&gt;Disclaimer: Who am I?&lt;&#x2F;b&gt;&lt;&#x2F;summary&gt;
&lt;p&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;clux&quot;&gt;I&lt;&#x2F;a&gt; am a platform engineer working maintenance of observability infrastructure, and a maintainer of &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;kube.rs&quot;&gt;kube-rs&lt;&#x2F;a&gt; working on rust integration of observability with Kubernetes platforms. My knowledge of prometheus is superficial and mostly based on practical observations around operating it for years. Suggestions or corrections are welcome (links at bottom).&lt;&#x2F;p&gt;
&lt;&#x2F;details&gt;
&lt;h2 id=&quot;signals-symptoms-causes&quot;&gt;Signals, Symptoms &amp;amp; Causes&lt;&#x2F;h2&gt;
&lt;p&gt;To illustrate this, let&#x27;s try to answer the (perhaps obvious question): &lt;strong&gt;why do you install a metrics system at all?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Primarily; we want to be able to track and get notified on changes to key &lt;strong&gt;signals&lt;&#x2F;strong&gt;. At the very basic level you want to tell if your &quot;service is down&quot;, because you want to be able to use it. That&#x27;s the main, user-facing signal. Setup something that test if the service responds with a &lt;code&gt;2XX&lt;&#x2F;code&gt;, and alert on deviations. You &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.checklyhq.com&#x2F;product&#x2F;api-monitoring&#x2F;&quot;&gt;don&#x27;t even need&lt;&#x2F;a&gt; prometheus for this.&lt;&#x2F;p&gt;
&lt;p&gt;However, while you can do basic sanity outside the cluster, you need &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.brendangregg.com&#x2F;usemethod.html&quot;&gt;utilisation and saturation&lt;&#x2F;a&gt;, to tell you about less obvious &#x2F; upcoming failures:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;message queues full&lt;&#x2F;strong&gt; :: rejected work&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;high cpu utilisation&lt;&#x2F;strong&gt; :: degraded latency&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;memory utilisation high&lt;&#x2F;strong&gt; :: oom kill imminent&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;disk space nearing full&lt;&#x2F;strong&gt; :: failures imminent&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;You can argue &lt;strong&gt;idealistically&lt;&#x2F;strong&gt; about whether you should only be &quot;aware of something going wrong&quot;, or be &quot;aware that something is in a risky state&quot; (e.g. &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;cloud.google.com&#x2F;blog&#x2F;topics&#x2F;developers-practitioners&#x2F;why-focus-symptoms-not-causes&quot;&gt;symptoms not causes&lt;&#x2F;a&gt;), but utilisation&#x2F;saturation&#x2F;errors are ultimately very good indications&#x2F;predictors for degraded performance, so we will focus on these herein.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-many-signals&quot;&gt;How Many Signals&lt;&#x2F;h2&gt;
&lt;blockquote&gt;
&lt;p&gt;MAIN POINT: You should be able to enumerate the &lt;strong&gt;basic&lt;&#x2F;strong&gt; signals that you want to have visibility of.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Let&#x27;s do some simplified &lt;strong&gt;enumeration maths&lt;&#x2F;strong&gt; on &lt;strong&gt;how many signals&lt;&#x2F;strong&gt; you actually want to properly identify failures quickly.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;compute-utilisation-saturation&quot;&gt;Compute Utilisation&#x2F;Saturation&lt;&#x2F;h3&gt;
&lt;p&gt;Consider an &lt;strong&gt;example cluster with 200 pods, and 5 nodes&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Want {utilization,saturation} of {cpu,memory} at cluster level :: 2 * 2 = &lt;strong&gt;4 signals&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Want to break these down per &lt;code&gt;Pod&lt;&#x2F;code&gt; :: 200 * 4 = &lt;strong&gt;800 signals&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Want to break these down per &lt;code&gt;Node&lt;&#x2F;code&gt; :: 5 * 4 = &lt;strong&gt;20 signals&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;So, in theory, we should be able to visualise cluster, node, and pod level utilisation for cpu and memory with only 820 metrics (but likely more if you want to break down node metrics).&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;NB: Pods are only found on one node, so &lt;code&gt;Pod&lt;&#x2F;code&gt; cardinality does not multiply with &lt;code&gt;Node&lt;&#x2F;code&gt; cardinality.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;These will come from a combination of &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;cadvisor&#x2F;blob&#x2F;master&#x2F;docs&#x2F;storage&#x2F;prometheus.md&quot;&gt;cadvisor&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kube-state-metrics&quot;&gt;kube-state-metrics&lt;&#x2F;a&gt;; huge beasts with lots of functionality and way more signals than we need. Depending on how much of the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;monitoring.mixins.dev&#x2F;kubernetes&#x2F;&quot;&gt;kubernetes mixin&lt;&#x2F;a&gt; you want, you may want more signals.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;node-state-breakdown&quot;&gt;Node State Breakdown&lt;&#x2F;h3&gt;
&lt;p&gt;If you want to break down things &lt;strong&gt;within a node&lt;&#x2F;strong&gt; on a more physical level, then you you can also grab &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;node_exporter&quot;&gt;node-exporter&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Assuming, for simplicity, 10 cores per node, 10 disk devices per node, and 10 network interfaces:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Want {utilization,saturation,errors} of cpu :: 3*10 * 5 = &lt;strong&gt;60 signals&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Want {utilization,saturation,errors} for memory :: 3*5 = &lt;strong&gt;15 signals&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Want {utilisation,saturation,errors} of disks :: 3*10 * 5 = &lt;strong&gt;100 signals&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Want {utilisation,saturation,errors} of network interfaces :: 10*3 * 5 = &lt;strong&gt;150 signals&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;In theory, you should be able to get decent node monitoring with less than 400 metrics.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;in-practice&quot;&gt;In Practice&lt;&#x2F;h2&gt;
&lt;p&gt;Doing this type of enumeration is helpful as a way to tell how close you are to your theoretical 100% optimised system, but how does the number hold up in practice?&lt;&#x2F;p&gt;
&lt;h3 id=&quot;metric-inefficiencies&quot;&gt;Metric Inefficiencies&lt;&#x2F;h3&gt;
&lt;p&gt;The problems with expecting this type of perfection in practice is that many metric producers are very inefficient &#x2F; overly lenient with their output. You can click on the addendum below for some examples, but without extreme tuning you can minimally expect a &lt;strong&gt;5x inefficiency factor&lt;&#x2F;strong&gt; to apply to the above in practice.&lt;&#x2F;p&gt;
&lt;details&gt;&lt;summary style=&quot;cursor:pointer;color:#0af&quot;&gt;&lt;b&gt;Addendum: Inefficiency Examples&lt;&#x2F;b&gt;&lt;&#x2F;summary&gt;
&lt;p&gt;Take for instance, &lt;code&gt;job=&quot;node-exporter&quot;&lt;&#x2F;code&gt; putting tons of labels on metrics such as &lt;code&gt;node_cpu_seconds_total&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;promql&quot; class=&quot;language-promql z-code&quot;&gt;&lt;code class=&quot;language-promql&quot; data-lang=&quot;promql&quot;&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;{cpu=&amp;quot;15&amp;quot;, mode=&amp;quot;idle&amp;quot;} 1
&lt;&#x2F;span&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;{cpu=&amp;quot;15&amp;quot;, mode=&amp;quot;iowait&amp;quot;} 1
&lt;&#x2F;span&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;{cpu=&amp;quot;15&amp;quot;, mode=&amp;quot;irq&amp;quot;} 1
&lt;&#x2F;span&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;{cpu=&amp;quot;15&amp;quot;, mode=&amp;quot;nice&amp;quot;} 1
&lt;&#x2F;span&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;{cpu=&amp;quot;15&amp;quot;, mode=&amp;quot;softirq&amp;quot;} 1
&lt;&#x2F;span&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;{cpu=&amp;quot;15&amp;quot;, mode=&amp;quot;steal&amp;quot;} 1
&lt;&#x2F;span&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;{cpu=&amp;quot;15&amp;quot;, mode=&amp;quot;system&amp;quot;} 1
&lt;&#x2F;span&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;{cpu=&amp;quot;15&amp;quot;, mode=&amp;quot;user&amp;quot;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Maybe you dont care about either of these splits, you just want to have a &lt;code&gt;cpu&lt;&#x2F;code&gt; total and data for one &lt;code&gt;mode&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Well, firstly, it&#x27;s hard to get rid of &lt;code&gt;mode&lt;&#x2F;code&gt; because the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;node_exporter&#x2F;blob&#x2F;b9d0932179a0c5b3a8863f3d6cdafe8584cedc8e&#x2F;docs&#x2F;node-mixin&#x2F;rules&#x2F;rules.libsonnet#L9-L14&quot;&gt;standard recording rules depend on mode&lt;&#x2F;a&gt;. Secondly, while you could try to rid of cpu, there&#x27;s no great way of doing this without something like &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;last9.io&#x2F;blog&#x2F;streaming-aggregation-vs-recording-rules&#x2F;&quot;&gt;stream aggregation&lt;&#x2F;a&gt; as &lt;code&gt;labeldrop: cpu&lt;&#x2F;code&gt; leads to collisions.&lt;&#x2F;p&gt;
&lt;p&gt;Similarly, if you want to support a bunch of the mixin dashboards with container level breakdown, you are also forced to grab a bunch of container level info for e.g. &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;71845c4d5795ec552e3ea96036c39dcfb97132ad&#x2F;charts&#x2F;kube-prometheus-stack&#x2F;templates&#x2F;prometheus&#x2F;rules-1.14&#x2F;k8s.rules.container_resource.yaml#L43&quot;&gt;k8s.rules.container_resource&lt;&#x2F;a&gt; unless if you want to rewrite the world.&lt;&#x2F;p&gt;
&lt;p&gt;And on top of this, many of these exporters also export things in very suboptimal ways. E.g. &lt;code&gt;job=&quot;kube-state-metrics&quot;&lt;&#x2F;code&gt; denormalising &lt;strong&gt;disjoint phases&lt;&#x2F;strong&gt; (never letting old&#x2F;zero phases go out of scope):&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;promql&quot; class=&quot;language-promql z-code&quot;&gt;&lt;code class=&quot;language-promql&quot; data-lang=&quot;promql&quot;&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;kube_pod_status_phase{phase=&amp;quot;Pending&amp;quot;, pod=&amp;quot;forgejo-975b98575-fbjz8&amp;quot;} 0
&lt;&#x2F;span&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;kube_pod_status_phase{phase=&amp;quot;Succeeded&amp;quot;, pod=&amp;quot;forgejo-975b98575-fbjz8&amp;quot;} 0
&lt;&#x2F;span&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;kube_pod_status_phase{phase=&amp;quot;Failed&amp;quot;, pod=&amp;quot;forgejo-975b98575-fbjz8&amp;quot;} 0
&lt;&#x2F;span&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;kube_pod_status_phase{phase=&amp;quot;Unknown&amp;quot;, pod=&amp;quot;forgejo-975b98575-fbjz8&amp;quot;} 0
&lt;&#x2F;span&gt;&lt;span class=&quot;z-text z-plain&quot;&gt;kube_pod_status_phase{phase=&amp;quot;Running&amp;quot;, pod=&amp;quot;forgejo-975b98575-fbjz8&amp;quot;} 1
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This particular one &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kube-state-metrics&#x2F;issues&#x2F;2380&quot;&gt;may get a fix&lt;&#x2F;a&gt; though.&lt;&#x2F;p&gt;
&lt;&#x2F;details&gt;
&lt;h3 id=&quot;visualising-cardinality&quot;&gt;Visualising Cardinality&lt;&#x2F;h3&gt;
&lt;p&gt;I have a &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;clux&#x2F;homelab&#x2F;blob&#x2F;main&#x2F;dashboards&#x2F;prometheus.json&quot;&gt;dashboard&lt;&#x2F;a&gt; for this with this panel:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;cardinality-control.png&quot;&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;cardinality-control.png&quot; alt=&quot;cardinality control panel&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Shows metrics produced, how many we decided to keep, and a breakdown link leading to &lt;code&gt;topk(50, count({job=&quot;JOB-IN-ROW&quot;}) by (__name__))&lt;&#x2F;code&gt;. It&#x27;s probably the panel that has helped the most in slimming down prometheus.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;outing-out&quot;&gt;Outing Out&lt;&#x2F;h3&gt;
&lt;p&gt;The most unfortunate reality is that this stuff is all opt-out. The &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;ecc58b9baecc43b0d5719ed509a89e7ca5a7e8e3&#x2F;charts&#x2F;kube-prometheus-stack&#x2F;values.yaml#L47-L83&quot;&gt;defaults are crazy inclusive&lt;&#x2F;a&gt;, and new versions of exporters introduce new metrics forcing you to watch your usage graph upgrades.&lt;&#x2F;p&gt;
&lt;p&gt;Thankfully, in a &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;clux&#x2F;homelab&quot;&gt;gitops repo&lt;&#x2F;a&gt; this is easy to do (if you &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;hachyderm.io&#x2F;@clux&#x2F;113044641297413034&quot;&gt;actually generate your helm templates&lt;&#x2F;a&gt; into a &lt;code&gt;deploy&lt;&#x2F;code&gt; folder) because all your dashboards and recording rules live there.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Procedure&lt;&#x2F;strong&gt;: Unsure if you need this &lt;code&gt;container_sockets&lt;&#x2F;code&gt; metric? &lt;code&gt;rg container_sockets deploy&#x2F;&lt;&#x2F;code&gt; and see if anything comes up:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;No hits? &lt;code&gt;action: drop&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Only used in a dashboard? Do you care about this dashboard? No? &lt;code&gt;action: drop&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Used in a recording rule? Does the recording rule go towards an alert&#x2F;dashboard you care about? No? &lt;code&gt;action: drop&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Grafana cloud has its own opt-out ML based &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ZkXJIQYbUVs&quot;&gt;adaptive metrics thing&lt;&#x2F;a&gt; to do this, and it&#x27;s probably helpful if you are locked into their cloud. The solution definitely has a big engineering solution &lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;adaptive-metric-power-meme.png&quot;&gt;feel&lt;&#x2F;a&gt; to it, but can&#x27;t really blame it as it&#x27;s not like you can easily remove polluting labels from needed metrics these days without causing collisions in OSS prom.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;things-you-don-t-need&quot;&gt;Things You Don&#x27;t Need&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;sub-minute-data-fidelity&quot;&gt;Sub Minute Data Fidelity&lt;&#x2F;h3&gt;
&lt;p&gt;You are probably checking your homelab once every few days at most, so why do you expect you would need 15s&#x2F;30s data fidelity? You can set &lt;code&gt;60s&lt;&#x2F;code&gt; scrape&#x2F;eval intervals and be fine:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;yaml&quot; class=&quot;language-yaml z-code&quot;&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;&lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;scrapeInterval&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;60s&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;&lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;evaluationInterval&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;60s&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;You could go even higher, but above &lt;code&gt;1m&lt;&#x2F;code&gt; grafana does starts to look less clean.&lt;&#x2F;p&gt;
&lt;p&gt;To compensate, you &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;clux&#x2F;homelab&#x2F;blob&#x2F;main&#x2F;justfile&quot;&gt;can hack your mixin dashboards&lt;&#x2F;a&gt; to increase the time window, and set a less aggressive refresh:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh z-code&quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;z-source z-shell z-bash&quot;&gt;&lt;span class=&quot;z-comment z-line z-number-sign z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-begin z-shell&quot;&gt;#&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-comment z-line z-number-sign z-shell&quot;&gt; change default refreshes on grafana dashboards to be 1m rather than 10s&lt;&#x2F;span&gt;&lt;span class=&quot;z-comment z-line z-number-sign z-shell&quot;&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-shell z-bash&quot;&gt;&lt;span class=&quot;z-meta z-function-call z-shell&quot;&gt;&lt;span class=&quot;z-variable z-function z-shell&quot;&gt;fd&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-shell&quot;&gt;&lt;span class=&quot;z-variable z-parameter z-option z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-parameter z-shell&quot;&gt; -&lt;&#x2F;span&gt;g&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-quoted z-single z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;*.yaml&lt;span class=&quot;z-punctuation z-definition z-string z-end z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; deploy&#x2F;promstack&#x2F;promstack&#x2F;charts&#x2F;kube-prometheus-stack&#x2F;templates&#x2F;grafana&#x2F;dashboards-1.14&#x2F; &lt;span class=&quot;z-punctuation z-separator z-continuation z-line z-shell&quot;&gt;\
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-shell z-bash&quot;&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-shell&quot;&gt;&lt;span class=&quot;z-variable z-parameter z-option z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-parameter z-shell&quot;&gt;   -&lt;&#x2F;span&gt;x&lt;&#x2F;span&gt; sd &lt;span class=&quot;z-string z-quoted z-single z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&amp;quot;refresh&amp;quot;:&amp;quot;10s&amp;quot;&lt;span class=&quot;z-punctuation z-definition z-string z-end z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-quoted z-single z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&amp;quot;refresh&amp;quot;:&amp;quot;1m&amp;quot;&lt;span class=&quot;z-punctuation z-definition z-string z-end z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-group z-expansion z-brace z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-expansion z-brace z-begin z-shell&quot;&gt;{&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-expansion z-brace z-end z-shell&quot;&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-shell z-bash&quot;&gt;&lt;span class=&quot;z-comment z-line z-number-sign z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-begin z-shell&quot;&gt;#&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-comment z-line z-number-sign z-shell&quot;&gt; change default time range to be now-12h rather than now-1h on boards that cannot handle 2 days...&lt;&#x2F;span&gt;&lt;span class=&quot;z-comment z-line z-number-sign z-shell&quot;&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-shell z-bash&quot;&gt;&lt;span class=&quot;z-meta z-function-call z-shell&quot;&gt;&lt;span class=&quot;z-variable z-function z-shell&quot;&gt;fd&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-shell&quot;&gt;&lt;span class=&quot;z-variable z-parameter z-option z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-parameter z-shell&quot;&gt; -&lt;&#x2F;span&gt;g&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-quoted z-single z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;*.yaml&lt;span class=&quot;z-punctuation z-definition z-string z-end z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; deploy&#x2F;promstack&#x2F;promstack&#x2F;charts&#x2F;kube-prometheus-stack&#x2F;templates&#x2F;grafana&#x2F;dashboards-1.14&#x2F; &lt;span class=&quot;z-punctuation z-separator z-continuation z-line z-shell&quot;&gt;\
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-shell z-bash&quot;&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-shell&quot;&gt;&lt;span class=&quot;z-variable z-parameter z-option z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-parameter z-shell&quot;&gt;   -&lt;&#x2F;span&gt;x&lt;&#x2F;span&gt; sd &lt;span class=&quot;z-string z-quoted z-single z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&amp;quot;from&amp;quot;:&amp;quot;now-1h&amp;quot;&lt;span class=&quot;z-punctuation z-definition z-string z-end z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-quoted z-single z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&amp;quot;from&amp;quot;:&amp;quot;now-12h&amp;quot;&lt;span class=&quot;z-punctuation z-definition z-string z-end z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-group z-expansion z-brace z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-expansion z-brace z-begin z-shell&quot;&gt;{&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-expansion z-brace z-end z-shell&quot;&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-shell z-bash&quot;&gt;&lt;span class=&quot;z-comment z-line z-number-sign z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-begin z-shell&quot;&gt;#&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-comment z-line z-number-sign z-shell&quot;&gt; change default time range to be now-2d rather than now-1h on solid dashboards...&lt;&#x2F;span&gt;&lt;span class=&quot;z-comment z-line z-number-sign z-shell&quot;&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-shell z-bash&quot;&gt;&lt;span class=&quot;z-meta z-function-call z-shell&quot;&gt;&lt;span class=&quot;z-variable z-function z-shell&quot;&gt;fd&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-shell&quot;&gt;&lt;span class=&quot;z-variable z-parameter z-option z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-parameter z-shell&quot;&gt; -&lt;&#x2F;span&gt;g&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-quoted z-single z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;k8s*.yaml&lt;span class=&quot;z-punctuation z-definition z-string z-end z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; deploy&#x2F;promstack&#x2F;promstack&#x2F;charts&#x2F;kube-prometheus-stack&#x2F;templates&#x2F;grafana&#x2F;dashboards-1.14&#x2F; &lt;span class=&quot;z-punctuation z-separator z-continuation z-line z-shell&quot;&gt;\
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-shell z-bash&quot;&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-shell&quot;&gt;&lt;span class=&quot;z-variable z-parameter z-option z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-parameter z-shell&quot;&gt;   -&lt;&#x2F;span&gt;x&lt;&#x2F;span&gt; sd &lt;span class=&quot;z-string z-quoted z-single z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&amp;quot;from&amp;quot;:&amp;quot;now-12h&amp;quot;&lt;span class=&quot;z-punctuation z-definition z-string z-end z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-quoted z-single z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&amp;quot;from&amp;quot;:&amp;quot;now-2d&amp;quot;&lt;span class=&quot;z-punctuation z-definition z-string z-end z-shell&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-group z-expansion z-brace z-shell&quot;&gt;&lt;span class=&quot;z-punctuation z-section z-expansion z-brace z-begin z-shell&quot;&gt;{&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-section z-expansion z-brace z-end z-shell&quot;&gt;}&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;..which is &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;clux&#x2F;homelab&#x2F;blob&#x2F;ff02315f3280c8199451160ab82a8e35a48f5cb1&#x2F;justfile#L36-L51&quot;&gt;actually practical&lt;&#x2F;a&gt; if you use &lt;code&gt;helm template&lt;&#x2F;code&gt; rather than &lt;code&gt;helm upgrade&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;ðŸ‘º: Somewhere out there, &lt;code&gt;jsonnet&lt;&#x2F;code&gt; users are gouging their eyes out reading this.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;most-kubelet-metrics&quot;&gt;Most Kubelet Metrics&lt;&#x2F;h3&gt;
&lt;p&gt;97% of kubelet metrics are junk. In a small cluster it&#x27;s the biggest waste producer in a small cluster, often producing 10x more metrics than anything else. Look at the top 10 metrics they produce with just 1 node and 30 pods:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;kublet-defaults-1-node.png&quot;&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;kublet-defaults-1-node.png&quot; alt=&quot;kubelet metrics top 10&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;None of these are useful &#x2F; contribute towards my above goal. Number 4 and 5 on that list &lt;strong&gt;together&lt;&#x2F;strong&gt; produce more signals than I consume in TOTAL in my actual setup. The &lt;code&gt;kube-prometheus-stack&lt;&#x2F;code&gt; chart does &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;ecc58b9baecc43b0d5719ed509a89e7ca5a7e8e3&#x2F;charts&#x2F;kube-prometheus-stack&#x2F;values.yaml#L1331-L1456&quot;&gt;attempt to reduce this somewhat&lt;&#x2F;a&gt;, but it by far too little.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;histograms&quot;&gt;Histograms&lt;&#x2F;h3&gt;
&lt;p&gt;Histograms generally account for a huge percentage of your metric utilisation. On a default &lt;code&gt;kube-prometheus-stack&lt;&#x2F;code&gt; install with one node, the distribution is &amp;gt;70% histograms:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;histogram-defaults.png&quot;&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;histogram-defaults.png&quot; alt=&quot;histograms vs non histograms on default kube-prometheus-stack &amp;gt; 70%&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Of course, this is largely due to &lt;code&gt;kubelet&lt;&#x2F;code&gt; metric inefficiencies, but it&#x27;s a trend you will see repeated elsewhere (thankfully usually with less eye-watering percentages).&lt;&#x2F;p&gt;
&lt;p&gt;Unless you have a need to see&#x2F;debug detailed breakdown of latency distributions, do not ingest or produce these metrics (see addendum for more info). It&#x27;s the easiest win you&#x27;ll get in prometheus land; wildcard drop them from your &lt;code&gt;ServiceMonitor&lt;&#x2F;code&gt; (et al) objects:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;yaml&quot; class=&quot;language-yaml z-code&quot;&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;      &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;metricRelabelings&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;      &lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;sourceLabels&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-flow-sequence z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-sequence z-begin z-yaml&quot;&gt;[&lt;&#x2F;span&gt;&lt;span class=&quot;z-string z-unquoted z-plain z-in z-yaml&quot;&gt;__name__&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-definition z-sequence z-end z-yaml&quot;&gt;]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;        &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;action&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;drop&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;        &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;regex&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-quoted z-single z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-yaml&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;^.*bucket.*&lt;span class=&quot;z-punctuation z-definition z-string z-end z-yaml&quot;&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;details&gt;&lt;summary style=&quot;cursor:pointer;color:#0af&quot;&gt;&lt;b&gt;Addendum: Why does histograms suck?&lt;&#x2F;b&gt;&lt;&#x2F;summary&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Multiplicative Cardinality&lt;&#x2F;strong&gt;: histogram buckets multiply metric cardinality by 5-30x (the number of buckets).&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;This number is multiplicative with regular cardinality:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pod&lt;&#x2F;code&gt; &#x2F; &lt;code&gt;node&lt;&#x2F;code&gt; labels added by prometheus operator :: if 2000 metric per pod, and scaling to 20 pods, now you have 40000 metrics&lt;&#x2F;li&gt;
&lt;li&gt;request information added by users (endpoint, route, status, error) :: 5 eps * 10 routes * 8 statuses * 4 errors = 1600 metrics, but with 30 buckets = 48000&lt;&#x2F;li&gt;
&lt;li&gt;See the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;promcon.io&#x2F;2019-munich&#x2F;slides&#x2F;containing-your-cardinality.pdf&quot;&gt;Containing Your Cardinality&lt;&#x2F;a&gt; talk for more maths details&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;&lt;strong&gt;Inefficiency&lt;&#x2F;strong&gt; 30x multiplied information is a bad way to store a few additional signals&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;If you want P50s or P99s you can compute these in the app with things like &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Moving_average&quot;&gt;rolling averages&lt;&#x2F;a&gt; or rolling quantiles. Some of these are more annoying than others, but there&#x27;s a lot you can do by just tracking a mean.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Strive for one signal per question where possible.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;That said, if you do actually need them, try to decouple them from your other labels (drop pod labels, drop peripheral information) so that you can get the answer you need cheaply. A histogram should answer one question, if your histogram has extra parameters, you can break them down to smaller histograms (addition beats multiplication for cardinality)&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;ðŸ‘º: Histograms will get better with the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;prometheus.io&#x2F;docs&#x2F;prometheus&#x2F;latest&#x2F;feature_flags&#x2F;#native-histograms&quot;&gt;native-histograms feature&lt;&#x2F;a&gt; (see e.g. &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=TgINvIK9SYc&quot;&gt;this talk&lt;&#x2F;a&gt;). However, this requires the ecosystem to move on to protobufs and it being propagated into client libraries (and is at the moment still experimental at the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;releases&#x2F;tag&#x2F;v2.54.1&quot;&gt;time of writing&lt;&#x2F;a&gt;), it&#x27;s only been &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;commit&#x2F;41035469d32fe8fd436c55846a5b237a86e69dee&quot;&gt;2 years&lt;&#x2F;a&gt; though.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;&#x2F;details&gt;
&lt;h2 id=&quot;a-solution&quot;&gt;A Solution&lt;&#x2F;h2&gt;
&lt;p&gt;Because I keep needing an efficient, low-cost homelab setup for prometheus (that still has the signals I care about), so now here is a chart.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s mostly a wrapper chart over &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;tree&#x2F;main&#x2F;charts&#x2F;kube-prometheus-stack&quot;&gt;kube-prometheus-stack&lt;&#x2F;a&gt; with aggressive tunings &#x2F; dropping (of what can&#x27;t be tuned), and it provides the following default &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;clux&#x2F;homelab&#x2F;blob&#x2F;main&#x2F;charts&#x2F;promstack&#x2F;values.yaml&quot;&gt;values.yaml&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;You &lt;strong&gt;could&lt;&#x2F;strong&gt; use it directly with:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;sh&quot; class=&quot;language-sh z-code&quot;&gt;&lt;code class=&quot;language-sh&quot; data-lang=&quot;sh&quot;&gt;&lt;span class=&quot;z-source z-shell z-bash&quot;&gt;&lt;span class=&quot;z-meta z-function-call z-shell&quot;&gt;&lt;span class=&quot;z-variable z-function z-shell&quot;&gt;helm&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-shell&quot;&gt; repo add clux https:&#x2F;&#x2F;clux.github.io&#x2F;homelab&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-shell z-bash&quot;&gt;&lt;span class=&quot;z-meta z-function-call z-shell&quot;&gt;&lt;span class=&quot;z-variable z-function z-shell&quot;&gt;helm&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-meta z-function-call z-arguments z-shell&quot;&gt; install &lt;span class=&quot;z-keyword z-control z-regexp z-set z-begin z-shell&quot;&gt;[&lt;&#x2F;span&gt;RELEASE_NAME&lt;span class=&quot;z-keyword z-control z-regexp z-set z-end z-shell&quot;&gt;]&lt;&#x2F;span&gt; clux&#x2F;promstack&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;but more sensibly, you can take&#x2F;dissect the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;clux&#x2F;homelab&#x2F;blob&#x2F;main&#x2F;charts&#x2F;promstack&#x2F;values.yaml&quot;&gt;values.yaml&lt;&#x2F;a&gt; file and run with it in your own similar subchart.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;ðŸ‘º: You shouldn&#x27;t trust me for maintenance of this, and you don&#x27;t want to be any more steps abstracted away from &lt;code&gt;kube-prometheus-stack&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;architecture-diagram&quot;&gt;Architecture Diagram&lt;&#x2F;h2&gt;
&lt;p&gt;The chart is effectively a slimmed down &lt;a href=&quot;&#x2F;post&#x2F;2022-01-11-prometheus-ecosystem&#x2F;&quot;&gt;2022 thanos setup&lt;&#x2F;a&gt;; no HA, no &lt;code&gt;thanos&lt;&#x2F;code&gt;, no &lt;code&gt;prometheus-adapter&lt;&#x2F;code&gt;, but a including a lightweight &lt;code&gt;tempo&lt;&#x2F;code&gt; for exemplars:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;prometheus-simple.png&quot;&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;prometheus-simple.png&quot; alt=&quot;prometheus architecture diagram&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;features&quot;&gt;Features&lt;&#x2F;h2&gt;
&lt;p&gt;A low-compromises prometheus, all the signals you need at near minimum cost.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;cardinality-control&quot;&gt;Cardinality Control&lt;&#x2F;h3&gt;
&lt;p&gt;Drops &lt;strong&gt;97% of kubelet metrics&lt;&#x2F;strong&gt;, configures minimal &lt;code&gt;kube-state-metrics&lt;&#x2F;code&gt;, &lt;code&gt;node-exporter&lt;&#x2F;code&gt; (with some cli arg configurations and some relabelling based drops) and a few other apps. In the end we have a cluster running with &lt;strong&gt;7000 metrics&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;cardinality-control.png&quot;&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;cardinality-control.png&quot; alt=&quot;cardinality control panel&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s higher than my idealistic estimate - particularly considering this is a one node cluster atm. It can definitely be tuned lower with extra effort, but this is a good checkpoint. This is low enough that &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;pricing&#x2F;&quot;&gt;grafana cloud considers it free&lt;&#x2F;a&gt;. So is it?&lt;&#x2F;p&gt;
&lt;h3 id=&quot;low-utilization&quot;&gt;Low Utilization&lt;&#x2F;h3&gt;
&lt;p&gt;The end result is a prometheus averaging &lt;code&gt;&amp;lt;0.01&lt;&#x2F;code&gt; cores and with &lt;code&gt;&amp;lt;512Mi&lt;&#x2F;code&gt; memory use over a week with &lt;code&gt;30d&lt;&#x2F;code&gt; retention:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;minimal-prom.png&quot; alt=&quot;cpu &#x2F; memory utilisation 7d means&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;auxilary-features&quot;&gt;Auxilary Features&lt;&#x2F;h3&gt;
&lt;ol&gt;
&lt;li&gt;Tempo for &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kube-rs&#x2F;controller-rs&#x2F;pull&#x2F;72#issuecomment-2335150121&quot;&gt;exemplars&lt;&#x2F;a&gt;, so we can cross-link from metric panels to grafana&#x27;s trace viewer.&lt;&#x2F;li&gt;
&lt;li&gt;Metrics Server for basic HPA scaling and &lt;code&gt;kubectl top&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;blockquote&gt;
&lt;p&gt;ðŸ‘º: You technically don&#x27;t need &lt;code&gt;metrics-server&lt;&#x2F;code&gt; if you are in an unscalable homelab, but having access to &lt;code&gt;kubectl top&lt;&#x2F;code&gt; is nice. Another avenue for homelab scaling is &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;keda.sh&#x2F;&quot;&gt;keda&lt;&#x2F;a&gt; with its &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;keda.sh&#x2F;docs&#x2F;2.15&#x2F;reference&#x2F;scaledobject-spec&#x2F;#minreplicacount&quot;&gt;scale to zero&lt;&#x2F;a&gt; ability.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;cx-dashboards&quot;&gt;CX Dashboards&lt;&#x2F;h3&gt;
&lt;p&gt;The screenshots here are from my own homebrew &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;clux&#x2F;homelab&#x2F;tree&#x2F;main&#x2F;charts&#x2F;cx-dashboards&quot;&gt;cx-dashboards&lt;&#x2F;a&gt; released separately. See the future post for these.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;links-comments&quot;&gt;Links &#x2F; Comments&lt;&#x2F;h2&gt;
&lt;p&gt;Posted on &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;hachyderm.io&#x2F;@clux&#x2F;113097824893646159&quot;&gt;mastodon&lt;&#x2F;a&gt;. Feel free to comment &#x2F; &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;clux&#x2F;homelab&#x2F;issues&quot;&gt;raise issues&lt;&#x2F;a&gt; &#x2F; &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;clux&#x2F;probes&#x2F;edit&#x2F;main&#x2F;content&#x2F;post&#x2F;2024-09-07-prometheus-minified.md&quot;&gt;suggest an edit&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>You Will (Not) Scale Prometheus</title>
        <published>2024-08-15T00:00:00+00:00</published>
        <updated>2024-08-15T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://clux.dev/post/2024-08-15-thanos-misadventures-with-scaling/"/>
        <id>https://clux.dev/post/2024-08-15-thanos-misadventures-with-scaling/</id>
        
        <content type="html" xml:base="https://clux.dev/post/2024-08-15-thanos-misadventures-with-scaling/">&lt;p&gt;To aid a memory obese &lt;code&gt;prometheus&lt;&#x2F;code&gt;, I recently helped in attempting to slowly shift a cluster over to &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;prometheus.io&#x2F;blog&#x2F;2021&#x2F;11&#x2F;16&#x2F;agent&#x2F;&quot;&gt;prometheus agent mode&lt;&#x2F;a&gt; sending data to &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;components&#x2F;receive.md&#x2F;&quot;&gt;thanos receive&lt;&#x2F;a&gt; over the last couple of months. I have now personally given up on this goal due to a variety of reasons, and this post explores why.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;background-setup&quot;&gt;Background Setup&lt;&#x2F;h2&gt;
&lt;p&gt;The original setup we started out with is basically this (via &lt;a href=&quot;&#x2F;post&#x2F;2022-01-11-prometheus-ecosystem&#x2F;&quot;&gt;2022 ecosystem post&lt;&#x2F;a&gt;):
&lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;ecosystem-miro.webp&quot;&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;ecosystem-miro.webp&quot; alt=&quot;prometheus architecture diagram&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;and we were planning to move to this:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;agentmode.webp&quot;&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;agentmode.webp&quot; alt=&quot;prometheus architecture diagram agent mode&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The key changes here:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;enable &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;prometheus.io&#x2F;docs&#x2F;prometheus&#x2F;latest&#x2F;feature_flags&#x2F;#prometheus-agent&quot;&gt;prometheus agent feature&lt;&#x2F;a&gt; limiting it to scraping and remote writes to receive&lt;&#x2F;li&gt;
&lt;li&gt;deploy &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;components&#x2F;receive.md&#x2F;&quot;&gt;thanos receive&lt;&#x2F;a&gt; for short term metric storage + S3 uploader (no more sidecar)&lt;&#x2F;li&gt;
&lt;li&gt;deploy &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;components&#x2F;rule.md&#x2F;&quot;&gt;thanos rule&lt;&#x2F;a&gt; as new evaluator, posting to alertmanager&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;With the 3 components replacing prometheus (agent, receive, ruler) in-theory having better scaling characteristics by themselves, with a cleaner, and more delineated area of responsibility.&lt;&#x2F;p&gt;
&lt;p&gt;Why chase better scaling characteristics? A single prometheus grows in size&#x2F;requests with amount of time series it scrapes, and it can only grow as long as you have enough RAM available. Eventually you run out of super-sized cloud nodes to run them. Have personally had to provision a 300GB memory node during a cardinality explosion, and would like to not deal with this ticking time bomb in the future.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;complexity-pickup&quot;&gt;Complexity &amp;amp; Pickup&lt;&#x2F;h2&gt;
&lt;p&gt;While the original setup can hardly be considered trivial, splitting one component into 3 sounds like a simple addition in theory.&lt;&#x2F;p&gt;
&lt;p&gt;However, while splitting a monolith might sound like a nice idea, actually operating such a distributed monolith is a different proposition. The existing complexity is splattered across 3 components in statefulsets and helm template gunk, and the operational complexity is compounded by these components not being as battle tested as the more traditional prometheus&#x2F;thanos setup.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-single-points-of-failure&quot;&gt;3 Single Points of Failure&lt;&#x2F;h3&gt;
&lt;p&gt;You will need improved alert coverage with the previous single point of failure getting split into 3 parts:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;components&#x2F;rule.md&#x2F;#risk&quot;&gt;ruler query failures&lt;&#x2F;a&gt; means no alerts get evaluated even though metrics exists in the system.&lt;&#x2F;li&gt;
&lt;li&gt;agent mode downtime means evaluation works, but metrics likely absent&lt;&#x2F;li&gt;
&lt;li&gt;receive write failures &#x2F; downtime means rule evaluation will fail&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The setup suffers from the &quot;who alerts about alerting failures&quot; problem. A single &lt;strong&gt;deadman&#x27;s switch&lt;&#x2F;strong&gt; is necessary, but not sufficient; as &lt;code&gt;ruler&lt;&#x2F;code&gt; can successfully send the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;d2566648d72d0ed136a38254985ccd25d6f894b8&#x2F;charts&#x2F;kube-prometheus-stack&#x2F;templates&#x2F;prometheus&#x2F;rules-1.14&#x2F;general.rules.yaml#L55-L88&quot;&gt;WatchDog alert&lt;&#x2F;a&gt; to your external ping service, despite the agent being down.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;monitoring.mixins.dev&#x2F;thanos&#x2F;#thanos-receive&quot;&gt;mixins&lt;&#x2F;a&gt; provide a good starting point for the new thanos components that can be adapted, but it takes a little time to grok it all.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;configuration-splits&quot;&gt;Configuration Splits&lt;&#x2F;h3&gt;
&lt;p&gt;Since &lt;code&gt;thanos-ruler&lt;&#x2F;code&gt; is the new query evaluator, and we use &lt;code&gt;PrometheusRule&lt;&#x2F;code&gt; crds, these crds must be provisioned into &lt;code&gt;thanos-ruler&lt;&#x2F;code&gt; via &lt;code&gt;prometheus-operator&lt;&#x2F;code&gt;. On the helm side, this only works with &lt;code&gt;kube-prometheus-stack&lt;&#x2F;code&gt; creating the &lt;code&gt;ThanosRuler&lt;&#x2F;code&gt; crd (which &lt;code&gt;prometheus-operator&lt;&#x2F;code&gt; will use to generate &lt;code&gt;thanos-ruler&lt;&#x2F;code&gt;), in the same way this chart normally creates the &lt;code&gt;Prometheus&lt;&#x2F;code&gt; &#x2F; &lt;code&gt;PrometheusAgent&lt;&#x2F;code&gt; crd (to generate the &lt;code&gt;prometheus&lt;&#x2F;code&gt; or &lt;code&gt;prometheus-agent&lt;&#x2F;code&gt; pair).&lt;&#x2F;p&gt;
&lt;p&gt;Specifically, we have to NOT enable &lt;code&gt;ruler&lt;&#x2F;code&gt; from the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bitnami&#x2F;charts&#x2F;blob&#x2F;main&#x2F;bitnami&#x2F;thanos&#x2F;README.md&quot;&gt;bitnami&#x2F;thanos&lt;&#x2F;a&gt; chart, and have a thanos component live inside &lt;code&gt;kube-prometheus-stack&lt;&#x2F;code&gt; instead. Not a major stumbling block, but goes to show some of the many sources of confusion&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;There&#x27;s a bigger stumbling block for &lt;code&gt;thanos-receive&lt;&#x2F;code&gt;, but more on that in the speculation section.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;new-features-slow-iteration&quot;&gt;New Features, Slow Iteration&lt;&#x2F;h3&gt;
&lt;p&gt;Agent mode, with a writing ruler also feels fairly new (in prometheus time), and support for all the features generally takes a long time to fully propagate from prometheus, to thanos, to the operator.&lt;&#x2F;p&gt;
&lt;p&gt;As an example see; &lt;code&gt;keep_firing_for&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Nov 2022 :: &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;issues&#x2F;11570&quot;&gt;Raised in prometheus&lt;&#x2F;a&gt; (me, lazy)&lt;&#x2F;li&gt;
&lt;li&gt;Feb 2023 :: &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;releases&#x2F;tag&#x2F;v2.42.0&quot;&gt;Implemented in prometheus&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;June 2023 :: &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-operator&#x2F;prometheus-operator&#x2F;releases&#x2F;tag&#x2F;v0.66.0&quot;&gt;Support in prometheus-operator for PrometheusRule&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Jan 2024 :: &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thanos-io&#x2F;thanos&#x2F;releases&#x2F;tag&#x2F;v0.34.0&quot;&gt;Support in thanos&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;March 2024 :: &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-operator&#x2F;prometheus-operator&#x2F;releases&#x2F;tag&#x2F;v0.72.0&quot;&gt;Support in prometheus-operator for ThanosRuler&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;So as you can see, it&#x27;s a long chain where &lt;code&gt;ruler&lt;&#x2F;code&gt; sits at the very end, and &lt;strong&gt;to me&lt;&#x2F;strong&gt; this it is indicative of the amount of use &lt;code&gt;ruler&lt;&#x2F;code&gt; realistically gets. To drive that home, I&#x27;ve also had to upstream &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;pull&#x2F;4092&quot;&gt;remote write ruler functionality in the chart&lt;&#x2F;a&gt;, and my minor &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thanos-io&#x2F;thanos&#x2F;issues&#x2F;created_by&#x2F;clux&quot;&gt;issues in thanos&lt;&#x2F;a&gt; sit untouched.&lt;&#x2F;p&gt;
&lt;p&gt;Anyway, not really trying to shame these projects, things take time, and volunteer work is volunteer work. But the clear outcome here is that many features are not necessarily very battle tested.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;performance-problems&quot;&gt;Performance Problems&lt;&#x2F;h2&gt;
&lt;p&gt;Unfortunately, the performance from this setup (after weeks of tuning) was still &lt;strong&gt;2x-3x worse&lt;&#x2F;strong&gt; than the original HA prometheus pair setup (again from the &lt;a href=&quot;&#x2F;post&#x2F;2022-01-11-prometheus-ecosystem&#x2F;&quot;&gt;2022 post&lt;&#x2F;a&gt; &#x2F; 1st diagram above). These new subcomponents individually perform worse than the original prometheus, and have worse scaling characteristics. Rule evaluation performance also seriously deteriorated.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;benchmark&quot;&gt;Benchmark&lt;&#x2F;h3&gt;
&lt;p&gt;Comparison is made using prometheus &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;releases&#x2F;tag&#x2F;v2.53.1&quot;&gt;v2.53.1&lt;&#x2F;a&gt; and thanos &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thanos-io&#x2F;thanos&#x2F;releases&#x2F;tag&#x2F;v0.36.0&quot;&gt;0.36.0&lt;&#x2F;a&gt; and consider &lt;code&gt;mean&lt;&#x2F;code&gt; utilisation measurements from cadvisor metrics on a cluster with a &lt;code&gt;~2.5M&lt;&#x2F;code&gt; time series per prometheus replica. We only consider the biggest statefulsets (receive, prometheus&#x2F;agent, ruler, storegw). In either setup &lt;code&gt;receive&lt;&#x2F;code&gt; or &lt;code&gt;prometheus&lt;&#x2F;code&gt; were running on a &lt;code&gt;3d&lt;&#x2F;code&gt; local retention.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;results&quot;&gt;Results&lt;&#x2F;h3&gt;
&lt;p&gt;Over full workdays we saw &lt;code&gt;~13 cores&lt;&#x2F;code&gt; constantly churning, and &lt;code&gt;~80 GB&lt;&#x2F;code&gt; of memory used by the 3 statefulsets (10 cores and 50GB alone from &lt;code&gt;receive&lt;&#x2F;code&gt;):&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;sts-load-agentmode.png&quot;&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;sts-load-agentmode.png&quot; alt=&quot;measurements for agent mode&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Compare to the same setup using a normal HA prometheus (no ruler, no receive, local eval) and we have &lt;code&gt;~4 cores&lt;&#x2F;code&gt; and &lt;code&gt;&amp;lt;30GB&lt;&#x2F;code&gt; memory:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;sts-load-regular.png&quot;&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;sts-load-regular.png&quot; alt=&quot;measurements for normal prometheus mode&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;So cluster wise, we end up with a between 2x-3x drop by switching back to non-agented, monolithic prometheus.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;receive-performance&quot;&gt;Receive Performance&lt;&#x2F;h3&gt;
&lt;p&gt;From the same graphs we see that the portion of prometheus that got factored out into thanos receive, is &lt;strong&gt;using roughly 2x the CPU and memory of a standalone prometheus&lt;&#x2F;strong&gt;, despite not doing any evaluation &#x2F; scraping.&lt;&#x2F;p&gt;
&lt;details&gt;&lt;summary style=&quot;cursor:pointer;color:#0af&quot;&gt;&lt;b&gt;Addendum: Configuration Attempts&lt;&#x2F;b&gt;&lt;&#x2F;summary&gt;
&lt;p&gt;Tried various flags here over many iterations to see if anything had any practical effects.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;components&#x2F;receive.md&#x2F;#ketama-recommended&quot;&gt;new ketama hashing algorithm&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;--enable-auto-gomemlimit&lt;&#x2F;code&gt; - barely helps&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;--tsdb.wal-compression&lt;&#x2F;code&gt; - disk benefit only afaikt&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;--receive.forward.async-workers=1000&lt;&#x2F;code&gt;  - irrelevant, receive does not forward requests in our setup&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The receiver was run as minimally with &lt;code&gt;3d&lt;&#x2F;code&gt; retention, and 1 replication factor, 2 replicas. More about this later.&lt;&#x2F;p&gt;
&lt;&#x2F;details&gt;
&lt;h3 id=&quot;agent-performance&quot;&gt;Agent Performance&lt;&#x2F;h3&gt;
&lt;p&gt;The agents (which now should only do scraping and remote write into receivers) are surprisingly not free either. From graph above, the memory utilisation is close to a full prometheus!&lt;&#x2F;p&gt;
&lt;p&gt;There is at least &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;issues&#x2F;10431&quot;&gt;one open related bug for this&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ruler-performance&quot;&gt;Ruler Performance&lt;&#x2F;h3&gt;
&lt;p&gt;Ruler evaluation performance when having to go through queriers is also impacted, and it surprisingly scales non-linearly with number of ruler replicas.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;ruler-time-by-pod.webp&quot; alt=&quot;ruler evaluation time per pod over 1h&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This panel evaluates &lt;code&gt;sum(avg_over_time(prometheus_rule_group_last_duration_seconds[1h])) by (pod)&lt;&#x2F;code&gt; per pod over three modes:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;up until 08&#x2F;06 12ish :: 2 replicas of thanos ruler&lt;&#x2F;li&gt;
&lt;li&gt;middle on 08&#x2F;07 :: 1 replica of thanos ruler&lt;&#x2F;li&gt;
&lt;li&gt;end on 08&#x2F;08 :: 2 replicas of prometheus (non-agent mode)&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;As you can see the query time increases within each pod when increasing replicas. Possibly this is load accumulating in the new distributed monolith, but a near 50% spike per ruler? In either case, the actual comparison of &lt;code&gt;3s avg&lt;&#x2F;code&gt; vs &lt;code&gt;50s avg&lt;&#x2F;code&gt; is kind of outrageous. Maybe this is misreading it, but the system definitely felt more sluggish in general.&lt;&#x2F;p&gt;
&lt;p&gt;No rules generally missed evaluations, but it got close to it, and that was not a good sign given it was in our low-load testing cluster.&lt;&#x2F;p&gt;
&lt;p&gt;Beyond this, this component is nice; seemingly not bad in terms of utilisation, and easy to debug for the basics. &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;components&#x2F;rule.md&#x2F;#must-have-essential-ruler-alerts&quot;&gt;Ruler metrics docs&lt;&#x2F;a&gt; and &lt;code&gt;sum(increase(prometheus_rule_evaluation_failures_total{}[1h])) by (pod)&lt;&#x2F;code&gt; in particular were very helpful.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;speculation&quot;&gt;Speculation&lt;&#x2F;h2&gt;
&lt;p&gt;..on why it performs like this, and on whether we are expecting too much from this setup.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;removing-colocation&quot;&gt;Removing Colocation&lt;&#x2F;h3&gt;
&lt;p&gt;This is tiny brain post-rationalisation, but maybe having a big block of memory directly available for 3 purposes (scrape &#x2F; storage &#x2F; eval) without having to go through 3 hops and buffer points (ruler â†’ query â†’ receive) is a big deal.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;split-receivers&quot;&gt;Split Receivers&lt;&#x2F;h3&gt;
&lt;p&gt;There is a lot more complexity under the surface of for actually running &lt;code&gt;receive&lt;&#x2F;code&gt; well. I ran the basic setup, and probably paid for it.&lt;&#x2F;p&gt;
&lt;p&gt;For people that need to go deeper; there&#x27;s a &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thanos-io&#x2F;thanos&#x2F;blob&#x2F;release-0.22&#x2F;docs&#x2F;proposals-accepted&#x2F;202012-receive-split.md&quot;&gt;split receiver setup&lt;&#x2F;a&gt;, and a &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;observatorium&#x2F;thanos-receive-controller&quot;&gt;third-party controller to manage its hashring&lt;&#x2F;a&gt; that people &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thanos-io&#x2F;thanos&#x2F;issues&#x2F;6784&quot;&gt;recommend to avoid write downtime&lt;&#x2F;a&gt; (not a problem I even noticed). By using it right, supposedly we get to &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thanos-io&#x2F;thanos&#x2F;issues&#x2F;7054#issuecomment-1933270766&quot;&gt;double the utilisation again&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;My lazy take here is that if the system performs badly with replication factor 1, the prospect of more complexity and a futher utilisation increase is not particularly inviting. Even if such a system scales, paying your way out of it with this much pointless compute resources feels wrong.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;bigger-evaluation-window&quot;&gt;Bigger Evaluation Window&lt;&#x2F;h3&gt;
&lt;p&gt;There is a chance that a good portion of the &lt;code&gt;ruler&lt;&#x2F;code&gt; time has come from going through &lt;code&gt;thanos-query&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Routing like this was a deliberate choice so that people could write alerts referencing more than &lt;code&gt;3d&lt;&#x2F;code&gt; (local retention) worth of data to do more advanced rules for anomaly detection. This &lt;strong&gt;should not&lt;&#x2F;strong&gt; have impacted most of our rules since most do not do this type of long range computations..&lt;&#x2F;p&gt;
&lt;p&gt;I tried moving the &lt;code&gt;ruler&lt;&#x2F;code&gt; query endpoint directly to &lt;code&gt;receive&lt;&#x2F;code&gt; to try to falsify this assumption, but this did not work from &lt;code&gt;ruler&lt;&#x2F;code&gt; using the same syntax as query&#x2F;storegw.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;wrong-tool-for-the-job&quot;&gt;Wrong Tool For the Job&lt;&#x2F;h3&gt;
&lt;p&gt;Agent mode on the prometheus side seems perhaps more geared to network gapped &#x2F; edge &#x2F; multi-cluster setups than what we were looking for judging by the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;prometheus.io&#x2F;blog&#x2F;2021&#x2F;11&#x2F;16&#x2F;agent&#x2F;&quot;&gt;grafana original announce&lt;&#x2F;a&gt; + &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;components&#x2F;receive.md&#x2F;#receiver&quot;&gt;thanos receive docs&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;p&gt;Itâ€™s also possible that other solutions perform better &#x2F; are better suited, e.g. grafana mimir. This is all speculation.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;confusing-agent-promise&quot;&gt;Confusing Agent Promise&lt;&#x2F;h2&gt;
&lt;p&gt;Perhaps the most confusing thing to me is that &lt;strong&gt;agent mode does not act like an agent&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;You cannot run it as a &lt;code&gt;DaemonSet&lt;&#x2F;code&gt;, you merely split the monolith out into a distributed monolith. This is a present-day limitation. Despite people willing to help out, the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-operator&#x2F;prometheus-operator&#x2F;issues&#x2F;5495&quot;&gt;issue&lt;&#x2F;a&gt; remains unmoving. I had hoped google would upstream its actual statefulset agent design (mentioned in the issue), but so far that has not materialised. Who knows if &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;discussions&#x2F;10979&quot;&gt;agent mode will even become stable&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;On the grafana cloud side, the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;docs&#x2F;agent&#x2F;latest&#x2F;static&#x2F;operation-guide&#x2F;&quot;&gt;grafana agent&lt;&#x2F;a&gt; did &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;grafana&#x2F;agent&#x2F;blob&#x2F;c281c76df02b7b1ce4d3c0192915628343f4c897&#x2F;operations&#x2F;helm&#x2F;charts&#x2F;grafana-agent&#x2F;templates&#x2F;controllers&#x2F;daemonset.yaml&quot;&gt;support running as a daemonset&lt;&#x2F;a&gt;, but &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;blog&#x2F;2024&#x2F;04&#x2F;09&#x2F;grafana-agent-to-grafana-alloy-opentelemetry-collector-faq&#x2F;&quot;&gt;it is now EOL&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Itâ€™s been &lt;em&gt;only&lt;&#x2F;em&gt; 3 years since &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;prometheus.io&#x2F;blog&#x2F;2021&#x2F;11&#x2F;16&#x2F;agent&#x2F;&quot;&gt;agent mode was announced&lt;&#x2F;a&gt;. Now, 2 years later the whole &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;issues&#x2F;13105&quot;&gt;remote write protocol is being updated&lt;&#x2F;a&gt; and &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;releases&#x2F;tag&#x2F;v2.54.0&quot;&gt;just landed in prometheus&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;So, what I am trying to say; who knows what the future really brings here.
It might be another couple of years before new remote write gets propagated through the thanos ecosysystem.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;scaling-alternatives&quot;&gt;Scaling Alternatives&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;sharding&quot;&gt;Sharding&lt;&#x2F;h3&gt;
&lt;p&gt;Maybe the better way forward for scaling is not to twist prometheus into something it&#x27;s not - and create a staggeringly complex system - but by making more prometheuses.&lt;&#x2F;p&gt;
&lt;p&gt;For instance; &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-operator&#x2F;prometheus-operator&#x2F;blob&#x2F;main&#x2F;Documentation&#x2F;user-guides&#x2F;shards-and-replicas.md&quot;&gt;prometheus operator&#x27;s sharding guide&lt;&#x2F;a&gt; can help partition a classic prometheus, but you do need partition and label management, uneven shard request (cpu&#x2F;mem) management (due to some shards scraping more metrics and thus having more load), so it&#x27;s definitely on the more manual side.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;brain: ..you would also need to split kubelet metrics across namespaces (or whatever you use as your shard) via some templated servicemonitor, and you&#x27;d need a bunch of templated datasources in your master grafana that your dashboards would need to be parametrised for. Maybe you also need one main-cluster prometheus that can scrape all the kubelet metrics for cluster-wide views.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Not impossible, but clearly also an amount of faff. This is standard configuration faff though; not distributed systems faff.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;lowering-local-retention&quot;&gt;Lowering Local Retention&lt;&#x2F;h3&gt;
&lt;p&gt;If the problem is delaying scaling up to something complex, we could also lean on the classic thanos split and keep reducing local prometheus &lt;code&gt;retention&lt;&#x2F;code&gt; time down to a single day or lower (as long as you are quick on detecting sidecar failures so you don&#x27;t lose data).&lt;&#x2F;p&gt;
&lt;p&gt;This is a temporary solution though. On my homelab I can run &lt;code&gt;30d&lt;&#x2F;code&gt; retention, but with 5M time series - in a company setting - I need &lt;code&gt;3d&lt;&#x2F;code&gt; to maintain a sensible utilisation.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;cardinality-enforcement&quot;&gt;Cardinality Enforcement&lt;&#x2F;h3&gt;
&lt;p&gt;This is the &quot;unscaling&quot; approach I run in my homelab. Granted it is easier to justify there, but there are real concrete steps you can do to really reduce the prometheus utilisation:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;drop big histograms (easy) &#x2F; move to native histograms (..some day)&lt;&#x2F;li&gt;
&lt;li&gt;dropping pod enrichment (big replica counts X histograms = lots of cardinality)&lt;&#x2F;li&gt;
&lt;li&gt;Monitor your ingestion: &lt;code&gt;by (job)&lt;&#x2F;code&gt;, before and after relabellings, put alerts on fixed ingestion numbers&lt;&#x2F;li&gt;
&lt;li&gt;Make sure everyone uses &lt;code&gt;{Service,Pod}Monitor&lt;&#x2F;code&gt;s so above step is feasible&lt;&#x2F;li&gt;
&lt;li&gt;Drop most of kubelet metrics (most metrics are unused by dashboards or mixin alerts)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I&#x27;ll probably explore this approach in more detail later on, because I think it&#x27;s the most sensible one; dilligence on the home court avoids all the complexity.&lt;&#x2F;p&gt;
&lt;p&gt;In the mean time, post is on &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;hachyderm.io&#x2F;@clux&#x2F;112967786148145839&quot;&gt;mastodon&lt;&#x2F;a&gt;, source is in the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;clux&#x2F;probes&#x2F;blob&#x2F;main&#x2F;content&#x2F;post&#x2F;2024-09-15-thanos-receive-scale-fail.md&quot;&gt;probes repo&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;future&quot;&gt;Future&lt;&#x2F;h2&gt;
&lt;p&gt;No matter how you slice it, agent mode with thanos is certainly a complex beast whose configuration entangles a huge number of services; agent, operator, receive, query, store, compactor, ruler, adapters, alertmanager, grafana. You have a choice in how difficult you make this.&lt;&#x2F;p&gt;
&lt;p&gt;The performance characteristics measured above, while not initially impressive to me, is one point, but the complexity of the setup is what pushes it over the edge for me. If the stack becomes so complex that the entire thing cannot be understood if one key person leaves, then I would consider that a failure. This was hard enough to explain before &lt;code&gt;receive&lt;&#x2F;code&gt; and agent mode.&lt;&#x2F;p&gt;
&lt;p&gt;If I have to wrangle with cardinality limits, relabellings, label enrichment, or create charts to multiply prometheuses, then this all seems more maintainable than &lt;code&gt;receive&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Prometheus Stack Review</title>
        <published>2022-01-11T00:00:00+00:00</published>
        <updated>2022-01-11T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://clux.dev/post/2022-01-11-prometheus-ecosystem/"/>
        <id>https://clux.dev/post/2022-01-11-prometheus-ecosystem/</id>
        
        <content type="html" xml:base="https://clux.dev/post/2022-01-11-prometheus-ecosystem/">&lt;p&gt;As part of my work life in the past year, a chunk of my day-to-day life has consisted of maintaining a &lt;code&gt;prometheus&lt;&#x2F;code&gt; installation on top of a sizable kubernetes cluster. My original feeling was &quot;this is not that bad with &lt;code&gt;kube-prometheus-stack&lt;&#x2F;code&gt;&quot;, but this sentiment has worsened somewhat with the realisation that more and more customizations and pieces were needed for large scale use. Half a year later (and 6+ charts deep), I thought I&#x27;d collect my thoughts on the ecosystem - from an operational perspective - with a rough architecture overview post.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;Information here is based on my own learnings. Some details &lt;strong&gt;might&lt;&#x2F;strong&gt; be wrong. Please submit an &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;clux&#x2F;probes&#x2F;issues&quot;&gt;issue&lt;&#x2F;a&gt; &#x2F; &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;clux&#x2F;probes&#x2F;edit&#x2F;master&#x2F;content&#x2F;post&#x2F;2022-01-11-prometheus-ecosystem.md&quot;&gt;fix&lt;&#x2F;a&gt; if you see anything glaring.&lt;&#x2F;li&gt;
&lt;li&gt;This post uses the classical open source &lt;code&gt;prometheus&lt;&#x2F;code&gt; setup with HA pairs and &lt;code&gt;thanos&lt;&#x2F;code&gt; on top. There are other promising setups such as agent mode with remote write.&lt;&#x2F;li&gt;
&lt;li&gt;We are following the most-standard &lt;code&gt;helm&lt;&#x2F;code&gt; approach and using charts directly (i.e. &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-operator&#x2F;kube-prometheus&#x2F;&quot;&gt;avoiding direct use of jsonnet&lt;&#x2F;a&gt;)&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;You can debate the last point, but if you are optimizing for &lt;strong&gt;user-editability&lt;&#x2F;strong&gt; of the prometheus-stack, then &lt;code&gt;jsonnet&lt;&#x2F;code&gt; is kind of the opposite of that - particularly when the rest of the cloud is installed with &lt;code&gt;helm&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;architecture-overview&quot;&gt;Architecture Overview&lt;&#x2F;h2&gt;
&lt;p&gt;The TL;DR image. Open it up in a new tab, and cycle between if you want to read about specific components below.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;ecosystem-miro.webp&quot;&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;ecosystem-miro.webp&quot; alt=&quot;prometheus ecosystem architecture diagram&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Legend&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;user&lt;&#x2F;strong&gt; components and the user &lt;strong&gt;read path&lt;&#x2F;strong&gt; is &lt;i style=&quot;color:green&quot;&gt;green&lt;&#x2F;i&gt;&lt;&#x2F;li&gt;
&lt;li&gt;prometheus&#x2F;thanos write path is &lt;i style=&quot;color:red&quot;&gt;red&lt;&#x2F;i&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;helm charts&lt;&#x2F;code&gt; are denoted with &lt;strong&gt;thick dashed lines&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;arrows flow &lt;strong&gt;from&lt;&#x2F;strong&gt; the instigator of the verb &lt;strong&gt;to&lt;&#x2F;strong&gt; the object acted upon&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;developer-interaction&quot;&gt;Developer Interaction&lt;&#x2F;h2&gt;
&lt;p&gt;A &lt;strong&gt;user&lt;&#x2F;strong&gt; &#x2F; developer on a kubernetes cluster with the prometheus stack installed can be expected to:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;develop applications with hpas&lt;&#x2F;strong&gt; and have them &lt;strong&gt;scraped&lt;&#x2F;strong&gt; by prometheus for metrics&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;create dashboards&lt;&#x2F;strong&gt; in grafana and save them as &lt;code&gt;ConfigMap&lt;&#x2F;code&gt; entries&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;create alerting rules&lt;&#x2F;strong&gt; to be triggered when metrics exceed thresholds (and maybe even tweak existing mixins)&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;query metrics&lt;&#x2F;strong&gt; directly on grafana&#x27;s explore and thanos&#x27;s queryfrontend&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;..and the user should not have to know too much about the complicated spaghetti setup that this diagram might give a scary impression of.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;part-1-kube-prometheus-stack&quot;&gt;Part 1: kube-prometheus-stack&lt;&#x2F;h2&gt;
&lt;p&gt;The blue dashed line represents a set of components that are commonly deployed together on kubernetes due to their interdependence, and these are managed together in the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;tree&#x2F;main&#x2F;charts&#x2F;kube-prometheus-stack&quot;&gt;kube-prometheus-stack&lt;&#x2F;a&gt; helm chart.&lt;&#x2F;p&gt;
&lt;p&gt;It is a &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;main&#x2F;charts&#x2F;kube-prometheus-stack&#x2F;values.yaml&quot;&gt;~3k LOC yaml values file&lt;&#x2F;a&gt; with a further 71k LOC of yaml in that chart folder alone (what could go wrong), and it configures the following components:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;prometheus&lt;&#x2F;li&gt;
&lt;li&gt;alertmanager&lt;&#x2F;li&gt;
&lt;li&gt;prometheus-operator&lt;&#x2F;li&gt;
&lt;li&gt;grafana&lt;&#x2F;li&gt;
&lt;li&gt;kube-state-metrics&lt;&#x2F;li&gt;
&lt;li&gt;node-exporter&lt;&#x2F;li&gt;
&lt;li&gt;kubernetes specific monitors&lt;&#x2F;li&gt;
&lt;li&gt;monitoring mixins&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;code&gt;24k&lt;&#x2F;code&gt; lines here are &lt;strong&gt;just&lt;&#x2F;strong&gt; the absolutely &lt;strong&gt;massive&lt;&#x2F;strong&gt; prometheus-operator crds (that are now &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;issues&#x2F;1500&quot;&gt;too big to apply&lt;&#x2F;a&gt;), but it&#x27;s still an astonishing amount of yaml. Typically you&#x27;ll end up with between &lt;code&gt;20-40k&lt;&#x2F;code&gt; (excluding the crds) with a 100-500 line values file that you have to maintain &lt;small&gt;(you generally don&#x27;t want your values file to be too large as it becomes harder and harder to keep track of the breaking changes in the stringly typed helm chart apis)&lt;&#x2F;small&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;prometheus&quot;&gt;prometheus&lt;&#x2F;h3&gt;
&lt;p&gt;The octopus at the base of our architecture. Prometheus &lt;strong&gt;scrapes&lt;&#x2F;strong&gt; the metrics endpoints of virtually &lt;strong&gt;every&lt;&#x2F;strong&gt; application you have, &lt;strong&gt;stores&lt;&#x2F;strong&gt; the data &lt;strong&gt;locally&lt;&#x2F;strong&gt; in a low-retention (a week or two) time series database that you can query. &lt;small&gt;(The grey scrape arrows are illustrative, whereas they would usually hit everything, and also hit it from every prometheus pod in the statefulset for redundancy).&lt;&#x2F;small&gt;&lt;&#x2F;p&gt;
&lt;p&gt;It also continually computes configured &lt;strong&gt;evaluation rules&lt;&#x2F;strong&gt;, and raises alerts on configured metric thresholds.&lt;&#x2F;p&gt;
&lt;p&gt;Prometheus is &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;commit&#x2F;734d28b515026ca9f429eba0a7d09954bceb6387&quot;&gt;over 9 years old&lt;&#x2F;a&gt;, and has &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.cncf.io&#x2F;projects&#x2F;prometheus&#x2F;&quot;&gt;graduate maturity in cncf&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;In theory, you can run it directly, tell it to scrape this-and-this and be done, but that will lead to downtime quickly. You are going to need at least two replicas for failover, and these are going to need mounted volumes to store their data. A &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;workloads&#x2F;controllers&#x2F;statefulset&#x2F;&quot;&gt;&lt;code&gt;StatefulSet&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; in kubernetes.&lt;&#x2F;p&gt;
&lt;p&gt;For configuration; how to scrape metrics can be tweaked through a mounted &lt;code&gt;scrape_config&lt;&#x2F;code&gt; (a &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;prometheus.io&#x2F;docs&#x2F;prometheus&#x2F;latest&#x2F;configuration&#x2F;configuration&#x2F;#scrape_config&quot;&gt;pretty complicated yaml DSL&lt;&#x2F;a&gt; using &lt;code&gt;snake_case&lt;&#x2F;code&gt;). If you get the syntax wrong, prometheus hates your config and won&#x27;t boot. &lt;code&gt;promtool&lt;&#x2F;code&gt; can validate it.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;yaml&quot; class=&quot;language-yaml z-code&quot;&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;&lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;relabel_configs&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;source_labels&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-flow-sequence z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-sequence z-begin z-yaml&quot;&gt;[&lt;&#x2F;span&gt;&lt;span class=&quot;z-string z-unquoted z-plain z-in z-yaml&quot;&gt;__meta_kubernetes_service_annotation_prometheus_io_scrape&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-definition z-sequence z-end z-yaml&quot;&gt;]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;action&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;keep&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;regex&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-language z-boolean z-yaml&quot;&gt;true&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;source_labels&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-flow-sequence z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-sequence z-begin z-yaml&quot;&gt;[&lt;&#x2F;span&gt;&lt;span class=&quot;z-string z-unquoted z-plain z-in z-yaml&quot;&gt;__meta_kubernetes_service_annotation_prometheus_io_scheme&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-definition z-sequence z-end z-yaml&quot;&gt;]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;action&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;replace&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;target_label&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;__scheme__&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;regex&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;(https?)&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;source_labels&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-flow-sequence z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-sequence z-begin z-yaml&quot;&gt;[&lt;&#x2F;span&gt;&lt;span class=&quot;z-string z-unquoted z-plain z-in z-yaml&quot;&gt;__meta_kubernetes_service_annotation_prometheus_io_path&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-definition z-sequence z-end z-yaml&quot;&gt;]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;action&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;replace&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;target_label&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;__metrics_path__&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;regex&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;(.+)&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;source_labels&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-flow-sequence z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-sequence z-begin z-yaml&quot;&gt;[&lt;&#x2F;span&gt;&lt;span class=&quot;z-string z-unquoted z-plain z-in z-yaml&quot;&gt;__address__&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-sequence z-yaml&quot;&gt;,&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-in z-yaml&quot;&gt;__meta_kubernetes_service_annotation_prometheus_io_port&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-definition z-sequence z-end z-yaml&quot;&gt;]&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;action&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;replace&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;target_label&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;__address__&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;regex&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;([^:]+)(?::\d+)?;(\d+)&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;replacement&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;$1:$2&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;action&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;labelmap&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;regex&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;__meta_kubernetes_service_label_(.+)&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This config is pretty awful to write and debug manually, so imo, you should probably avoid writing it yourself (see the operator below).&lt;&#x2F;p&gt;
&lt;p&gt;Consider importing the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;970e1334813f90348b849f0a3850262a61f82797&#x2F;charts&#x2F;prometheus&#x2F;values.yaml#L1516-L1759&quot;&gt;semi-standardised &lt;code&gt;prometheus.io&#x2F;scrape&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; ones from the main prometheus chart if you wish (they slightly clash with the root chart), but those should be it. Scrape config also needs you to inline secrets, so not great from a security perspective.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;prometheus.io&#x2F;docs&#x2F;prometheus&#x2F;latest&#x2F;configuration&#x2F;alerting_rules&#x2F;&quot;&gt;Alerting and recording rules&lt;&#x2F;a&gt; are similarly configured and has same caveats (don&#x27;t write them manually).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;alertmanager&quot;&gt;alertmanager&lt;&#x2F;h3&gt;
&lt;p&gt;Alerts (the data in the special &lt;code&gt;ALERTS{alertstate=&quot;firing&quot;}&lt;&#x2F;code&gt; metric) are sent from &lt;code&gt;prometheus&lt;&#x2F;code&gt; to &lt;code&gt;alertmanager&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;eva-alert.webp&quot; alt=&quot;how alerts should look&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;At least, this is usually what happens. The communication &lt;strong&gt;to&lt;&#x2F;strong&gt; and &lt;strong&gt;within&lt;&#x2F;strong&gt; &lt;code&gt;alertmanager&lt;&#x2F;code&gt; is probably the most &lt;strong&gt;annoying&lt;&#x2F;strong&gt; parts of this entire architecture.&lt;&#x2F;p&gt;
&lt;p&gt;A problem that keeps biting me is how &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;issues&#x2F;7063&quot;&gt;prometheus can lose track of alertmanager ips, and fail to send alerts for hours&lt;&#x2F;a&gt;. There are many &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;search?q=%22error+sending+alert%22&amp;amp;type=issues&quot;&gt;issues related to this&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;When alerts do actually get passed to alertmanager, they go through a &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;alertmanager&#x2F;blob&#x2F;main&#x2F;doc&#x2F;arch.svg&quot;&gt;pretty complicated internal architecture&lt;&#x2F;a&gt;, before ultimately being sent to &lt;strong&gt;configured receivers&lt;&#x2F;strong&gt;. Alertmanager contains &lt;strong&gt;deduplication&lt;&#x2F;strong&gt; mechanisms, and a &lt;strong&gt;custom&lt;&#x2F;strong&gt; UDP &amp;amp; TCP &lt;strong&gt;gossip protocol&lt;&#x2F;strong&gt; (that keeps breaking in HA setups - causing &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;alertmanager&#x2F;issues?q=is%3Aissue+duplicate+alerts&quot;&gt;duplicate alerts&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;p&gt;The built-in receivers for slack&#x2F;pagerduty seem to handle deduplicating alerts themselves, so if you can get by without HA and don&#x27;t need a custom webhook, you might be ok.&lt;&#x2F;p&gt;
&lt;p&gt;Still, &lt;strong&gt;your mileage will definitely vary&lt;&#x2F;strong&gt; with this component.&lt;&#x2F;p&gt;
&lt;p&gt;Alertmanager is &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;alertmanager&#x2F;commit&#x2F;f86966a0e75dfa52f068d3a085753518bd4aea74&quot;&gt;almost 9 years old&lt;&#x2F;a&gt;, has &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;alertmanager&#x2F;blob&#x2F;main&#x2F;MAINTAINERS.md&quot;&gt;2 maintainers&lt;&#x2F;a&gt;, and is a sub-project of the prometheus org.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;prometheus-operator&quot;&gt;prometheus-operator&lt;&#x2F;h3&gt;
&lt;p&gt;A system that sits on top of prometheus, and extends the configuration with the large &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-operator&#x2F;prometheus-operator&#x2F;tree&#x2F;main&#x2F;example&#x2F;prometheus-operator-crd&quot;&gt;&lt;code&gt;monitoring.coreos.com&lt;&#x2F;code&gt; CRDs&lt;&#x2F;a&gt;. This operator watches these CRDs, validates them via &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;reference&#x2F;access-authn-authz&#x2F;admission-controllers&#x2F;&quot;&gt;admission&lt;&#x2F;a&gt;, converts them to a prometheus compatible format, and injects them into the running prometheus via a &lt;code&gt;config-reloader&lt;&#x2F;code&gt; sidecar on the prometheus statefulset. The most commonly used configuration CRDs are &lt;code&gt;PrometheusRule&lt;&#x2F;code&gt; and &lt;code&gt;ServiceMonitor&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;This setup avoids you taking down prometheus when you&#x2F;users write new or invalid configuration or alerts. It also provides a mechanism for referencing kubernetes &lt;code&gt;Secret&lt;&#x2F;code&gt; objects (avoiding encryption needs on the base configuration), and it is generally considered the standard abstraction for configuring prometheus scraping.&lt;&#x2F;p&gt;
&lt;p&gt;A minor, but debatable choice they have made, is to re-map the &lt;code&gt;scrape_config&lt;&#x2F;code&gt; keys to &lt;code&gt;camelCase&lt;&#x2F;code&gt; forcing you have to write half of your &lt;code&gt;ServiceMonitor&lt;&#x2F;code&gt; in &lt;code&gt;camelCase&lt;&#x2F;code&gt; (the keys), and the other half in &lt;code&gt;snake_case&lt;&#x2F;code&gt; (the values):&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;yaml&quot; class=&quot;language-yaml z-code&quot;&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;    &lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;relabelings&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;      &lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;action&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;replace&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;        &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;sourceLabels&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;         &lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;__meta_kubernetes_namespace&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;         &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;targetLabel&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;kubernetes_namespace&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The operator &lt;strong&gt;configures&lt;&#x2F;strong&gt; &lt;code&gt;prometheus&lt;&#x2F;code&gt;, &lt;code&gt;alertmanager&lt;&#x2F;code&gt;, and optionally also thanos &lt;code&gt;ruler&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;It is over 5 years old and considered &lt;code&gt;beta&lt;&#x2F;code&gt;. It&#x27;s &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-operator&#x2F;prometheus-operator&#x2F;blob&#x2F;main&#x2F;MAINTAINERS.md&quot;&gt;maintained by&lt;&#x2F;a&gt; people who are mostly disjoint from the the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;blob&#x2F;main&#x2F;MAINTAINERS.md&quot;&gt;prometheus maintainers&lt;&#x2F;a&gt;. I.e. while this is not a first-class supported thing from prometheus, everyone uses it - and it is &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;prometheus-operator.dev&#x2F;docs&#x2F;prologue&#x2F;introduction&#x2F;&quot;&gt;well-documented&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;grafana&quot;&gt;grafana&lt;&#x2F;h3&gt;
&lt;p&gt;While prometheus does have sufficient querying functionality built in, it does not let you save these queries other than by a long-ass url, so realistically, users will want Grafana for dashboards and cool panel customization.&lt;&#x2F;p&gt;
&lt;p&gt;The key strength of Grafana lies in how it becomes the one-stop shop for &lt;strong&gt;querying &amp;amp; visualising anything&lt;&#x2F;strong&gt; when you buy into the ecosystem, and the huge amount of data sources they have available:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;docs&#x2F;grafana&#x2F;latest&#x2F;datasources&#x2F;prometheus&#x2F;#prometheus-api&quot;&gt;anything prometheus-like&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;grafana&#x2F;plugins&#x2F;cloudwatch&#x2F;&quot;&gt;cloudwatch&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;blog&#x2F;2021&#x2F;08&#x2F;30&#x2F;introducing-the-honeycomb-plugin-for-grafana&#x2F;&quot;&gt;honeycomb&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;docs&#x2F;tempo&#x2F;latest&#x2F;getting-started&#x2F;tempo-in-grafana&#x2F;&quot;&gt;tempo&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;docs&#x2F;loki&#x2F;latest&#x2F;getting-started&#x2F;grafana&#x2F;&quot;&gt;loki&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;blog&#x2F;2021&#x2F;03&#x2F;04&#x2F;why-were-partnering-with-elastic-to-build-the-elasticsearch-plugin-for-grafana&#x2F;&quot;&gt;elastic&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;blog&#x2F;2021&#x2F;12&#x2F;16&#x2F;introducing-the-sentry-data-source-plugin-for-grafana&#x2F;&quot;&gt;sentry&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;..plus tons more that you are less likely to run into (&lt;code&gt;cloudwatch&lt;&#x2F;code&gt; shown as one common case). Even if you only use if it against &lt;code&gt;prometheus&lt;&#x2F;code&gt;, it&#x27;s still a &lt;strong&gt;generally painless&lt;&#x2F;strong&gt; component to install with tons of &lt;strong&gt;benefits&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Grafana is packaged as a small-ish &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;grafana&#x2F;helm-charts&#x2F;tree&#x2F;main&#x2F;charts&#x2F;grafana&quot;&gt;grafana-maintained helm chart&lt;&#x2F;a&gt;, which is &lt;strong&gt;pinned&lt;&#x2F;strong&gt; as a &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;9401be121c65e6e3332670a49c5ad6ba2aeae9c3&#x2F;charts&#x2F;kube-prometheus-stack&#x2F;Chart.yaml#L45-L48&quot;&gt;subchart under kube-prometheus-stack&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The chart contains some nice ways of making &lt;strong&gt;dashboard provisioning&lt;&#x2F;strong&gt; automatic (dashboards as configmaps), but this comes with its own &lt;strong&gt;pain points&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;need to verify dashboard json out-of-band (validity + uid presence)&lt;&#x2F;li&gt;
&lt;li&gt;dashboards incur k8s size limits - 1MB on ConfigMap, 256kB applied annotation&lt;&#x2F;li&gt;
&lt;li&gt;HA setups could split brain dashboards with &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;grafana&#x2F;grafana&#x2F;issues&#x2F;37679&quot;&gt;partial saves&lt;&#x2F;a&gt; (seems fixed now)&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;grafana&#x2F;grafana&#x2F;issues&#x2F;36328&quot;&gt;provisioned dashboards are incompatible with built-in alerts&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;future &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;grafana&#x2F;grafana&#x2F;issues&#x2F;31038&quot;&gt;dashboard-as-code direction is very undecided&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Regardless, the diagram shows how the user flow would be for this, and how it ends up being picked up by a sidecar in the grafana statefulset.&lt;&#x2F;p&gt;
&lt;details&gt;&lt;summary style=&quot;cursor:pointer;color:#0af&quot;&gt;&lt;b&gt;Addendum: Governance &amp; Grafana Labs Sidenotes&lt;&#x2F;b&gt;&lt;&#x2F;summary&gt;
&lt;p&gt;Grafana has a more &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;grafana&#x2F;grafana&#x2F;blob&#x2F;main&#x2F;GOVERNANCE.md&quot;&gt;company driven governance model&lt;&#x2F;a&gt; - it&#x27;s maintained almost exclusively by people employed by Grafana Labs - and the company is clearly optimizing for their own cloud offering of a parallel subset of this ecosystem; &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;products&#x2F;cloud&#x2F;&quot;&gt;Grafana Cloud&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;This obvious &lt;em&gt;conflict of interest&lt;&#x2F;em&gt; does pollute the purity of ecosystem somewhat, but at least they have &lt;strong&gt;financing&lt;&#x2F;strong&gt; to move at the great pace they are moving. Some examples of their recent efforts:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;grafana&#x2F;loki&quot;&gt;loki&lt;&#x2F;a&gt; - a serious elastic contender for logs that integrates with grafana&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;grafana&#x2F;tempo&quot;&gt;tempo&lt;&#x2F;a&gt; - a super clean tracing backend that integrates with grafana&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;grafana&#x2F;helm-charts&#x2F;tree&#x2F;main&#x2F;charts&quot;&gt;well maintained helm charts for grafana&#x2F;tempo&#x2F;loki&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;blog&#x2F;2021&#x2F;11&#x2F;16&#x2F;why-we-created-a-prometheus-agent-mode-from-the-grafana-agent&#x2F;&quot;&gt;prometheus agent mode&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Of course, there is the expectation that open source functionality not related to grafana cloud &lt;strong&gt;might&lt;&#x2F;strong&gt; be receiving &lt;strong&gt;less attention&lt;&#x2F;strong&gt;, but I can&#x27;t really blame them for pursuing a sensible monetisation strategy.&lt;&#x2F;p&gt;
&lt;p&gt;I do hope &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;blog&#x2F;2021&#x2F;11&#x2F;09&#x2F;announcing-grafana-oncall&#x2F;&quot;&gt;Grafana OnCall&lt;&#x2F;a&gt; manages to get something contributed upstream (outside the grafana monolith) so we can have a better alternative to alertmanager (as alertmanager has lots of issues and can only alert on prometheus data sources).&lt;&#x2F;p&gt;
&lt;&#x2F;details&gt;
&lt;h3 id=&quot;monitoring-mixins&quot;&gt;Monitoring Mixins&lt;&#x2F;h3&gt;
&lt;p&gt;A default grafana&#x2F;prometheus-operator installation is not going to be very helpful without some dashboards that tell you about the state of your system(s).&lt;&#x2F;p&gt;
&lt;p&gt;Common patterns for alerts&#x2F;dashboards&#x2F;recording rules are encapsulated in a collection of mixins that are browsable on &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;monitoring.mixins.dev&#x2F;&quot;&gt;monitoring.mixins.dev&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;monitoring.mixins.dev&#x2F;kubernetes&#x2F;&quot;&gt;&lt;code&gt;kubernetes-mixin&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; stands out in particular, providing excellent, &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;docs&#x2F;grafana&#x2F;latest&#x2F;best-practices&#x2F;dashboard-management-maturity-levels&#x2F;&quot;&gt;high maturity&lt;&#x2F;a&gt;, drill-down-linked dashboards that are going to be vital for a large percentage of kubernetes related incidents.&lt;&#x2F;p&gt;
&lt;p&gt;In general, these provide a great starting point for most clusters (despite &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;monitoring.mixins.dev&#x2F;kubernetes&#x2F;#kubernetes-system-kubelet&quot;&gt;sometimes&lt;&#x2F;a&gt; being &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes-monitoring&#x2F;kubernetes-mixin&#x2F;search?q=KubeletTooManyPods&amp;amp;type=issues&quot;&gt;overly noisy&lt;&#x2F;a&gt;). You likely have to re-configure some &lt;strong&gt;thresholds&lt;&#x2F;strong&gt;, and remove some of these alerts as you see fit you your production cluster, but the defaults are generally intelligent.&lt;&#x2F;p&gt;
&lt;p&gt;Unfortunately, there are &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;grafana.com&#x2F;blog&#x2F;2021&#x2F;01&#x2F;14&#x2F;how-prometheus-monitoring-mixins-can-make-effective-observability-strategies-accessible-to-all&#x2F;#the-challenges-with-mixins&quot;&gt;many operational challenges with these mixins&lt;&#x2F;a&gt;. &lt;code&gt;helm&lt;&#x2F;code&gt; is certainly not best suited to take full advantage of them as not every option is bubbled up to the charts, and these values flow through so many layers it&#x27;s &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-operator&#x2F;kube-prometheus&#x2F;issues&#x2F;1333&quot;&gt;challenging&lt;&#x2F;a&gt; to find where they truly originate, e.g. &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;main&#x2F;charts&#x2F;kube-prometheus-stack&#x2F;templates&#x2F;prometheus&#x2F;rules-1.14&#x2F;prometheus.yaml&quot;&gt;kube-prometheus-stack&lt;&#x2F;a&gt; &amp;lt;- &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-operator&#x2F;kube-prometheus&#x2F;blob&#x2F;main&#x2F;manifests&#x2F;prometheusOperator-prometheusRule.yaml&quot;&gt;kube-prometheus&lt;&#x2F;a&gt; &amp;lt;- &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-operator&#x2F;prometheus-operator&#x2F;blob&#x2F;main&#x2F;example&#x2F;mixin&#x2F;alerts.yaml&quot;&gt;prometheus-operator&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;This often caused me not wanting to bother with fixing it in the first place, which of course leads to the mixins not being as good as they could be. AFAIKT, your options are:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;forking&lt;&#x2F;strong&gt; the mixins you care about and &lt;strong&gt;opting out&lt;&#x2F;strong&gt; of automatic upstream fixes&lt;&#x2F;li&gt;
&lt;li&gt;post-template modifications of minor details with hacky solutions like sed&#x2F;jq&lt;&#x2F;li&gt;
&lt;li&gt;managing mixins &lt;strong&gt;out-of-band&lt;&#x2F;strong&gt;, aligning implicit default values with the charts, and customizing with jsonnet&lt;&#x2F;li&gt;
&lt;li&gt;going through the drudgery of propagating mixin fixes through several repos&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;While I advocate for trying to propagate fixes when motivation strikes, &lt;strong&gt;forking mixins&lt;&#x2F;strong&gt; at least makes your yaml &lt;strong&gt;readable&lt;&#x2F;strong&gt; in your gitops repo, so it&#x27;s actually a decent option - particularly given how &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;prometheus-operator.dev&#x2F;docs&#x2F;developing-prometheus-rules-and-grafana-dashboards&#x2F;#adjustment&quot;&gt;awful&lt;&#x2F;a&gt; the other solutions are.&lt;&#x2F;p&gt;
&lt;p&gt;Regardless, it&#x27;s another useful, but imperfect component that you are going to need.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;metric-sources&quot;&gt;Metric Sources&lt;&#x2F;h3&gt;
&lt;p&gt;We move on to the components that create most of your data in prometheus.&lt;&#x2F;p&gt;
&lt;p&gt;Using the mixins (or equivalent dashboards) effectively &lt;strong&gt;requires&lt;&#x2F;strong&gt; you having &lt;strong&gt;standard kubernetes metric sources&lt;&#x2F;strong&gt; configured (otherwise you will have missing values in all your mixin dashboards).&lt;&#x2F;p&gt;
&lt;p&gt;We will briefly run through these metric sources, focusing first on the ones that appear in the &lt;code&gt;kube-prometheus-stack&lt;&#x2F;code&gt;. In general, these are pretty-well behaved and low-maintenance, so there won&#x27;t be too much to say about these.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;node-exporter&quot;&gt;node-exporter&lt;&#x2F;h4&gt;
&lt;p&gt;The main external metric source. A &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;node_exporter&quot;&gt;prometheus org maintained&lt;&#x2F;a&gt; &lt;code&gt;DaemonSet&lt;&#x2F;code&gt; component that scrapes system level &lt;strong&gt;unix metrics&lt;&#x2F;strong&gt;. It mounts &lt;code&gt;&#x2F;&lt;&#x2F;code&gt;, &lt;code&gt;&#x2F;sys&lt;&#x2F;code&gt;, and &lt;code&gt;&#x2F;proc&lt;&#x2F;code&gt; - with &lt;code&gt;hostPID&lt;&#x2F;code&gt; and &lt;code&gt;hostNetwork&lt;&#x2F;code&gt; enabled - to grab extensive information about each node.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s a &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;9401be121c65e6e3332670a49c5ad6ba2aeae9c3&#x2F;charts&#x2F;kube-prometheus-stack&#x2F;Chart.yaml#L41-L44&quot;&gt;sub-chart of kube-prometheus-stack&lt;&#x2F;a&gt;, and it has a slew of &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;node_exporter#collectors&quot;&gt;configurable exporters&lt;&#x2F;a&gt;, which can be &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;9401be121c65e6e3332670a49c5ad6ba2aeae9c3&#x2F;charts&#x2F;prometheus-node-exporter&#x2F;values.yaml#L150-L152&quot;&gt;configured from the chart&lt;&#x2F;a&gt;, but the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;9401be121c65e6e3332670a49c5ad6ba2aeae9c3&#x2F;charts&#x2F;kube-prometheus-stack&#x2F;values.yaml#L1358-L1360&quot;&gt;defaults from kube-prometheus-stack&lt;&#x2F;a&gt; are likely good enough.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;kube-state-metrics&quot;&gt;kube-state-metrics&lt;&#x2F;h4&gt;
&lt;p&gt;The second stand-alone metric exporter for kubernetes. Maintained by kubernetes itself; &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kube-state-metrics&quot;&gt;kube-state-metrics&lt;&#x2F;a&gt; is a &lt;strong&gt;smaller deployment&lt;&#x2F;strong&gt; that generates metrics from what it sees the state of objects are from the apiserver. It has &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kube-state-metrics&#x2F;blob&#x2F;master&#x2F;docs&#x2F;design&#x2F;metrics-store-performance-optimization.md#proposal&quot;&gt;client-go reflectors&lt;&#x2F;a&gt; and uses the results of their long watches to populate metrics.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s a conceptually pretty simple piece; an &lt;strong&gt;api -&amp;gt; metrics transformer&lt;&#x2F;strong&gt;, but kubernetes has a lot of apis, so definitely not something you want to write yourself.&lt;&#x2F;p&gt;
&lt;p&gt;In example terms; this component provides the &lt;strong&gt;base data&lt;&#x2F;strong&gt; for what you need to &lt;strong&gt;answer&lt;&#x2F;strong&gt; the questions like whether your:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&quot;&lt;code&gt;Pod&lt;&#x2F;code&gt; has been in an unhealthy state for &lt;code&gt;&amp;gt;N&lt;&#x2F;code&gt; minutes&quot;&lt;&#x2F;li&gt;
&lt;li&gt;&quot;&lt;code&gt;Deployment&lt;&#x2F;code&gt; has failed to complete its last rollout in &lt;code&gt;N&lt;&#x2F;code&gt; minutes&quot;&lt;&#x2F;li&gt;
&lt;li&gt;&quot;&lt;code&gt;HorizontalPodAutoscaler&lt;&#x2F;code&gt; has been maxed out for &lt;code&gt;&amp;gt;N&lt;&#x2F;code&gt; minutes&quot;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;..stuff that you can figure out with &lt;code&gt;kubectl get -oyaml&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;KSM is deployed via an &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;tree&#x2F;main&#x2F;charts&#x2F;kube-state-metrics&quot;&gt;in-tree subchart&lt;&#x2F;a&gt; under &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;9401be121c65e6e3332670a49c5ad6ba2aeae9c3&#x2F;charts&#x2F;kube-prometheus-stack&#x2F;Chart.yaml#L37-L40&quot;&gt;kube-prometheus-stack&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;You can configure what apis it provides metrics for &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;9401be121c65e6e3332670a49c5ad6ba2aeae9c3&#x2F;charts&#x2F;kube-state-metrics&#x2F;values.yaml#L155-L183&quot;&gt;under collectors&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;details&gt;&lt;summary style=&quot;cursor:pointer;color:#0af&quot;&gt;&lt;b&gt;Addendum: Label configuration caveat&lt;&#x2F;b&gt;&lt;&#x2F;summary&gt;
&lt;p&gt;The only issue I&#x27;ve run into with KSM is that the metric labels are often insufficient for integration with existing standard alerting setups (problem being that the generic alerts from &lt;code&gt;kubernetes-mixin&lt;&#x2F;code&gt; will fire, but it&#x27;s hard to tell by the name of the deployment alone who should get that alert). This can be rectified with the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;blob&#x2F;9401be121c65e6e3332670a49c5ad6ba2aeae9c3&#x2F;charts&#x2F;kube-state-metrics&#x2F;values.yaml#L135-L142&quot;&gt;&lt;code&gt;metricLabelsAllowlist&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; in the chart, e.g.:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;yaml&quot; class=&quot;language-yaml z-code&quot;&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;    &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;metricLabelsAllowlist&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;    &lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-quoted z-double z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-yaml&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;deployments=[app.kubernetes.io&#x2F;name,app]&lt;span class=&quot;z-punctuation z-definition z-string z-end z-yaml&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;    &lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-quoted z-double z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-yaml&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;jobs=[app.kubernetes.io&#x2F;name,app]&lt;span class=&quot;z-punctuation z-definition z-string z-end z-yaml&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;    &lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-quoted z-double z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-string z-begin z-yaml&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;horizontalpodautoscalers=[app.kubernetes.io&#x2F;name,app]&lt;span class=&quot;z-punctuation z-definition z-string z-end z-yaml&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;to inject more labels (here &lt;code&gt;app&lt;&#x2F;code&gt;) from the root object onto metrics. Annoyingly, these only get injected into an informational &lt;code&gt;_labels&lt;&#x2F;code&gt; metric, so you&#x27;d have to extend &lt;code&gt;kubernetes-mixin&lt;&#x2F;code&gt; with big joins to get these values exposed in the alert:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;diff&quot; class=&quot;language-diff z-code&quot;&gt;&lt;code class=&quot;language-diff&quot; data-lang=&quot;diff&quot;&gt;&lt;span class=&quot;z-source z-diff&quot;&gt;diff --git development&#x2F;rules-kubernetes-apps.yaml development&#x2F;rules-kubernetes-apps.yaml
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-diff&quot;&gt;index e36496e..988d6a6 100644
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-diff&quot;&gt;&lt;span class=&quot;z-meta z-diff z-header z-from-file&quot;&gt;&lt;span class=&quot;z-meta z-header z-from-file z-diff&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-from-file z-diff&quot;&gt;---&lt;&#x2F;span&gt; development&#x2F;rules-kubernetes-apps.yaml
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-diff&quot;&gt;&lt;span class=&quot;z-meta z-diff z-header z-to-file&quot;&gt;&lt;span class=&quot;z-meta z-header z-to-file z-diff&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-to-file z-diff&quot;&gt;+++&lt;&#x2F;span&gt; development&#x2F;rules-kubernetes-apps.yaml
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-diff&quot;&gt;&lt;span class=&quot;z-meta z-diff z-range z-unified&quot;&gt;&lt;span class=&quot;z-meta z-range z-unified z-diff&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-range z-diff&quot;&gt;@@&lt;&#x2F;span&gt; &lt;span class=&quot;z-meta z-toc-list z-line-number z-diff&quot;&gt;-220,7 +220,9&lt;&#x2F;span&gt; &lt;span class=&quot;z-punctuation z-definition z-range z-diff&quot;&gt;@@&lt;&#x2F;span&gt; &lt;span class=&quot;z-entity z-name z-section z-diff&quot;&gt;spec:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-diff&quot;&gt;&lt;span class=&quot;z-markup z-deleted z-diff&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-deleted z-diff&quot;&gt;-&lt;&#x2F;span&gt;      expr: kube_job_spec_completions{job=&amp;quot;kube-state-metrics&amp;quot;, namespace=~&amp;quot;.*&amp;quot;} - kube_job_status_succeeded{job=&amp;quot;kube-state-metrics&amp;quot;, namespace=~&amp;quot;.*&amp;quot;}  &amp;gt; 0
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-diff&quot;&gt;&lt;span class=&quot;z-markup z-inserted z-diff&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-inserted z-diff&quot;&gt;+&lt;&#x2F;span&gt;      expr: |-
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-diff&quot;&gt;&lt;span class=&quot;z-markup z-inserted z-diff&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-inserted z-diff&quot;&gt;+&lt;&#x2F;span&gt;        (kube_job_spec_completions{job=&amp;quot;kube-state-metrics&amp;quot;, namespace=~&amp;quot;.*&amp;quot;} -
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-diff&quot;&gt;&lt;span class=&quot;z-markup z-inserted z-diff&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-inserted z-diff&quot;&gt;+&lt;&#x2F;span&gt;        kube_job_status_succeeded{job=&amp;quot;kube-state-metrics&amp;quot;, namespace=~&amp;quot;.*&amp;quot;}  &amp;gt; 0) *
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-diff&quot;&gt;&lt;span class=&quot;z-markup z-inserted z-diff&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-inserted z-diff&quot;&gt;+&lt;&#x2F;span&gt;        on(job_name) group_left(label_app_kubernetes_io_name)
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-diff&quot;&gt;&lt;span class=&quot;z-markup z-inserted z-diff&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-inserted z-diff&quot;&gt;+&lt;&#x2F;span&gt;        sum by (namespace, job_name) (kube_job_labels{job=&amp;quot;kube-state-metrics&amp;quot;, label_app_kubernetes_io_name=~&amp;quot;.+&amp;quot;})
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;&#x2F;details&gt;
&lt;h4 id=&quot;kubernetes-internal-sources&quot;&gt;kubernetes internal sources&lt;&#x2F;h4&gt;
&lt;p&gt;For poking at the internals of kubernetes, you can enable configurable scrapers for:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;kubelet&lt;&#x2F;li&gt;
&lt;li&gt;kube-apiserver&lt;&#x2F;li&gt;
&lt;li&gt;kube-controller-manager&lt;&#x2F;li&gt;
&lt;li&gt;kube-scheduler&lt;&#x2F;li&gt;
&lt;li&gt;kube-proxy&lt;&#x2F;li&gt;
&lt;li&gt;coreDns or kubeDns&lt;&#x2F;li&gt;
&lt;li&gt;etcd&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;There is some sparse documentation for these under &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;cluster-administration&#x2F;system-metrics&#x2F;&quot;&gt;kubernetes&#x2F;cluster-admin&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;These are a little more optional, but you need the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes&#x2F;tree&#x2F;ea0764452222146c47ec826977f49d7001b0ea8c&#x2F;pkg&#x2F;kubelet&#x2F;metrics&#x2F;collectors&quot;&gt;kubelet metrics&lt;&#x2F;a&gt; (via &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;cadvisor&quot;&gt;cadvisor&lt;&#x2F;a&gt;), for the main kubernetes mixins, so make sure those are enabled.&lt;&#x2F;p&gt;
&lt;p&gt;The more superfluous ones that come with the kube-apiserver (particularly the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;monitoring.mixins.dev&#x2F;kubernetes&#x2F;#kube-apiserver-burnraterules&quot;&gt;burnrate stuff&lt;&#x2F;a&gt;) are particularly heavy evaluation rules (saw a ~6 cores reduction after removing them from one busy prometheus pair). Imo, you probably only need some of these if you are a cloud provider.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;extra-metrics-exporters&quot;&gt;Extra metrics exporters&lt;&#x2F;h4&gt;
&lt;p&gt;Any additional metrics exporters are not part of the &lt;code&gt;kube-prometheus-stack&lt;&#x2F;code&gt; chart, but there are &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;tree&#x2F;main&#x2F;charts&quot;&gt;tons of exporters availble&lt;&#x2F;a&gt; supported by the same &lt;code&gt;prometheus-community&lt;&#x2F;code&gt;, so would expect them to be of high quality based on their commit history and repository CI, but have otherwise not enough experience here.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;part-2-thanos&quot;&gt;Part 2: &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;&quot;&gt;Thanos&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Before this step, you can have a pretty self-contained prometheus stack where grafana&#x27;s default data source would point at the prometheus&#x27; &lt;code&gt;Service&lt;&#x2F;code&gt;, and metrics would fade out after prometheus&#x27; &lt;code&gt;retention&lt;&#x2F;code&gt; period.&lt;&#x2F;p&gt;
&lt;p&gt;Thanos essentially &lt;strong&gt;takes all the components&lt;&#x2F;strong&gt; that&#x27;s found &lt;strong&gt;inside prometheus&lt;&#x2F;strong&gt;, and allows you to deploy and &lt;strong&gt;scale them separately&lt;&#x2F;strong&gt;, while providing a &lt;strong&gt;prometheus compatible API&lt;&#x2F;strong&gt; for &lt;strong&gt;long term&lt;&#x2F;strong&gt; storage of metrics.&lt;&#x2F;p&gt;
&lt;p&gt;It relies on data being sent from any prometheus set - via a sidecar on prometheus (configurable via the &lt;code&gt;kube-prometheus-stack&lt;&#x2F;code&gt; chart) - to some provisioned object storage (here S3).&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;strong&gt;write paths&lt;&#x2F;strong&gt; to the S3 bucket is &lt;strong&gt;highlighted&lt;&#x2F;strong&gt; in &lt;i style=&quot;color:red&quot;&gt;red&lt;&#x2F;i&gt; on the diagram.&lt;&#x2F;p&gt;
&lt;p&gt;The various &lt;strong&gt;read paths&lt;&#x2F;strong&gt; are &lt;strong&gt;highlighted&lt;&#x2F;strong&gt; in &lt;i style=&quot;color:green&quot;&gt;green&lt;&#x2F;i&gt; on the diagram, and show how various types of reads propagate to various systems (grafana is the normal entrypoint, but the query-frontend is also a nice way to debug thanos specifics closer to the source).&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;strong&gt;thanos configuration&lt;&#x2F;strong&gt; is &lt;strong&gt;&quot;almost&quot;&lt;&#x2F;strong&gt; completely contained in the thanos &lt;strong&gt;chart&lt;&#x2F;strong&gt; (of your choice) and is marked with a dashed &lt;i style=&quot;color:purple&quot;&gt;purple&lt;&#x2F;i&gt; square.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&quot;almost&quot;&lt;&#x2F;strong&gt;: S3 configuration is also needed in &lt;code&gt;kube-prometheus-stack&lt;&#x2F;code&gt; for writing&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;There are several charts that are trying to package the same thing for thanos:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bitnami&#x2F;charts&#x2F;tree&#x2F;master&#x2F;bitnami&#x2F;thanos&#x2F;&quot;&gt;bitnami&#x2F;thanos&lt;&#x2F;a&gt; - most active&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;banzaicloud&#x2F;banzai-charts&#x2F;tree&#x2F;master&#x2F;thanos&quot;&gt;banzai&#x2F;thanos&lt;&#x2F;a&gt; - pretty inactive&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thanos-community&#x2F;helm-charts&quot;&gt;thanos-community&#x2F;helm-charts&lt;&#x2F;a&gt; - &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thanos-io&#x2F;thanos&#x2F;issues&#x2F;1820&quot;&gt;&quot;official&quot;&lt;&#x2F;a&gt;, but clearly abandoned&#x2F;out-of-date&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;They &lt;strong&gt;all have problems&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;bitnami&lt;&#x2F;code&gt; ties itself to its own ecosystem, and is not based on the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thanos-io&#x2F;kube-thanos&quot;&gt;official jsonnet&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;banzai&lt;&#x2F;code&gt; is clearly outdated&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;thanos-community&lt;&#x2F;code&gt; charts lacks developers (&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thanos-io&#x2F;thanos&#x2F;issues&#x2F;1820#issuecomment-752609815&quot;&gt;and they see helm users as a minority&lt;&#x2F;a&gt;)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;So far, the &lt;code&gt;bitnami&lt;&#x2F;code&gt; chart is the most appropriate for &lt;code&gt;helm&lt;&#x2F;code&gt; users.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s an evolving ecosystem with many components, but none of them are as complicated to operate as the &lt;code&gt;kube-prometheus-stack&lt;&#x2F;code&gt; components (and there seems to be a lot less footguns).&lt;&#x2F;p&gt;
&lt;p&gt;Thanos is an &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.cncf.io&#x2F;projects&#x2F;thanos&#x2F;&quot;&gt;incubating cncf project&lt;&#x2F;a&gt; that is just over &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thanos-io&#x2F;thanos&#x2F;commit&#x2F;3a7b2996f8574048900cfc6259561ac412bcf251&quot;&gt;4 years old&lt;&#x2F;a&gt;. It has a healthy set of &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thanos-io&#x2F;thanos&#x2F;blob&#x2F;main&#x2F;MAINTAINERS.md&quot;&gt;maintainers&lt;&#x2F;a&gt;, it &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.devstats.cncf.io&#x2F;d&#x2F;74&#x2F;contributions-chart?orgId=1&amp;amp;var-period=d7&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All&amp;amp;var-country_name=All&amp;amp;var-company_name=All&amp;amp;var-company=all&quot;&gt;moves fast&lt;&#x2F;a&gt;, and makes some of the most &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;thanos&#x2F;getting-started.md&#x2F;&quot;&gt;well-documented&lt;&#x2F;a&gt;, high-quality &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;thanos-io&#x2F;thanos&#x2F;releases&quot;&gt;releases&lt;&#x2F;a&gt; out there.&lt;&#x2F;p&gt;
&lt;p&gt;While it&#x27;s &lt;strong&gt;not trivial&lt;&#x2F;strong&gt; to maintain - the large cpu&#x2F;memory usage and scaling profiles presents some challenges - it has generally &lt;strong&gt;not&lt;&#x2F;strong&gt; presented major problems.&lt;&#x2F;p&gt;
&lt;p&gt;A quick run-through of things worth knowing about the components follows:&lt;&#x2F;p&gt;
&lt;h3 id=&quot;thanos-query-frontend&quot;&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;components&#x2F;query-frontend.md&#x2F;&quot;&gt;Thanos Query Frontend&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The http UI users can use. Very light-weight. The &lt;code&gt;Service&lt;&#x2F;code&gt; for this &lt;code&gt;Deployment&lt;&#x2F;code&gt; generally becomes the default user substitute way to query anything (instead of going to a prometheus service&#x27;s web interface - which after installing thanos is mostly useful for debugging scrape configs).&lt;&#x2F;p&gt;
&lt;p&gt;It also looks &lt;strong&gt;almost exactly&lt;&#x2F;strong&gt; like the prometheus web interface (sans ability to debug scrape targets).&lt;&#x2F;p&gt;
&lt;p&gt;This &lt;strong&gt;proxies&lt;&#x2F;strong&gt; all traffic to &lt;strong&gt;thanos query&lt;&#x2F;strong&gt; and never breaks.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;thanos-query&quot;&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;components&#x2F;query.md&#x2F;&quot;&gt;Thanos Query&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The big &lt;strong&gt;fan-out engine&lt;&#x2F;strong&gt; that fetches query data from one or more metric sources (typically thanos store + prometheus services), and computes the result of your query on the retrieved data.&lt;&#x2F;p&gt;
&lt;p&gt;This component will suddenly spike in both CPU and memory when it&#x27;s under heavy load (i.e. users doing big queries), so an HPA here on CPU works OK - albeit some stabilization values might be useful:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;yaml&quot; class=&quot;language-yaml z-code&quot;&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;scaleTargetRef&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;    &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;apiVersion&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;apps&#x2F;v1&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;    &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;kind&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;Deployment&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;    &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;name&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;prometheus-stack-thanos-query&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;minReplicas&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-yaml&quot;&gt;2&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;maxReplicas&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-yaml&quot;&gt;10&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-comment z-line z-number-sign z-yaml&quot;&gt;&lt;span class=&quot;z-punctuation z-definition z-comment z-line z-number-sign z-yaml&quot;&gt;#&lt;&#x2F;span&gt; Slow down scaleDown behavior as thanos query has very sporadic usage
&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;behavior&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;    &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;scaleDown&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;      &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;stabilizationWindowSeconds&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-yaml&quot;&gt;100&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;      &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;policies&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;      &lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;type&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;Percent&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;        &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;value&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-yaml&quot;&gt;10&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;        &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;periodSeconds&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-yaml&quot;&gt;60&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;    &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;scaleUp&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;      &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;policies&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;      &lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;type&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;Percent&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;        &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;value&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-yaml&quot;&gt;20&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;        &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;periodSeconds&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-yaml&quot;&gt;15&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;metrics&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;  &lt;span class=&quot;z-punctuation z-definition z-block z-sequence z-item z-yaml&quot;&gt;-&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;type&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;Resource&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;    &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;resource&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;      &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;name&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;cpu&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;      &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;target&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;        &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;averageUtilization&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-constant z-numeric z-integer z-decimal z-yaml&quot;&gt;70&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;span class=&quot;z-source z-yaml&quot;&gt;        &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;&lt;span class=&quot;z-entity z-name z-tag z-yaml&quot;&gt;type&lt;&#x2F;span&gt;&lt;&#x2F;span&gt;&lt;span class=&quot;z-punctuation z-separator z-key-value z-mapping z-yaml&quot;&gt;:&lt;&#x2F;span&gt; &lt;span class=&quot;z-string z-unquoted z-plain z-out z-yaml&quot;&gt;Utilization&lt;&#x2F;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The compute workload &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;monitoring.mixins.dev&#x2F;kubernetes&#x2F;#dashboards&quot;&gt;kubernetes-mixin dashboard&lt;&#x2F;a&gt; will show roughly how this HPA reacts to changes (stacked view, colors represent pods):&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;thanos-query-usage.png&quot;&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;thanos-query-usage.png&quot; alt=&quot;thanos query cpu and memory usage&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;which lines up with the &lt;code&gt;thanos_query_concurrent_gate_queries_in_flight&lt;&#x2F;code&gt; metric reasonably well.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;thanos-store&quot;&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;components&#x2F;store.md&#x2F;&quot;&gt;Thanos Store&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The read interface to long term storage. It&#x27;s also prometheus api compatible (using the Store API), so from the querier&#x27;s POV it is analogous to querying a prometheus.&lt;&#x2F;p&gt;
&lt;p&gt;This component also will &lt;strong&gt;also&lt;&#x2F;strong&gt; suddenly spike in both CPU and memory when users start doing big queries on historical data (i.e. further back in time than prometheus&#x27; &lt;code&gt;retention&lt;&#x2F;code&gt;), so a similar HPA to thanos query to scale on CPU works reasonably well:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;thanos-store-usage.png&quot;&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;thanos-store-usage.png&quot; alt=&quot;thanos store cpu and memory usage&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;thanos-ruler&quot;&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;components&#x2F;rule.md&#x2F;&quot;&gt;Thanos Ruler&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;An &lt;strong&gt;optional&lt;&#x2F;strong&gt; rule evaluation &#x2F; alerting analogue. Not pictured in the current image because it&#x27;s not really needed in the normal thanos setup (see the next &lt;a href=&quot;&#x2F;tags&#x2F;prometheus&#x2F;&quot;&gt;#prometheus&lt;&#x2F;a&gt; post for how this could be used).
This is a bit more &lt;strong&gt;niche&lt;&#x2F;strong&gt; than the rule evaluation in prometheus itself, because rule evalution on the prometheus side already gets stored as metrics in the long term storage. The only reasons you need this is if you need to alert &#x2F; evaluate rules on the federated level (e.g. to answer whether you have a high error rate across all production clusters &#x2F; prometheus sets), or need long term metrics in the evaluation phase for anomaly detection.&lt;&#x2F;p&gt;
&lt;p&gt;If you need the alerting part, then you have another component that talks to &lt;code&gt;alertmanager&lt;&#x2F;code&gt; ðŸ™ƒ.&lt;&#x2F;p&gt;
&lt;p&gt;Can run in a stateful mode - presenting a prometheus compatible store api that the querier can hit for rule results - or statelessly; persisting rule results to s3.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;thanos-compactor&quot;&gt;&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;components&#x2F;compact.md&#x2F;&quot;&gt;Thanos Compactor&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The magic sauce that makes &lt;strong&gt;querying long term&lt;&#x2F;strong&gt; data &lt;strong&gt;practical&lt;&#x2F;strong&gt; - as raw data is too resource intensive to use when you want to view result over the past weeks or months.&lt;&#x2F;p&gt;
&lt;p&gt;The compactor will go through the S3 bucket, and create lower-res data (at &lt;code&gt;5m&lt;&#x2F;code&gt; averages and &lt;code&gt;1h&lt;&#x2F;code&gt; averages), and delete raw data (after a configurable time has passed).&lt;&#x2F;p&gt;
&lt;p&gt;One such configuration can be:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Maintain &lt;code&gt;raw&lt;&#x2F;code&gt; resolution for &lt;code&gt;7d&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Create &lt;code&gt;5m&lt;&#x2F;code&gt; resolution chunks that are kept for &lt;code&gt;30d&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Create &lt;code&gt;1h&lt;&#x2F;code&gt; resolution chunks that are kept for &lt;code&gt;1y&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The &lt;code&gt;compactor&lt;&#x2F;code&gt; will &lt;strong&gt;chug along&lt;&#x2F;strong&gt; and do these in steps (creating &lt;code&gt;5m&lt;&#x2F;code&gt; res from raw, then creating &lt;code&gt;1h&lt;&#x2F;code&gt; res from &lt;code&gt;5m&lt;&#x2F;code&gt; res), as the chunks become available.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Long-term&lt;&#x2F;strong&gt;, the lower res data is &lt;strong&gt;more practical&lt;&#x2F;strong&gt; to both &lt;strong&gt;query&lt;&#x2F;strong&gt; and &lt;strong&gt;store&lt;&#x2F;strong&gt;, but you end up with multiple variants of the data in the first 7 to 30 days.&lt;&#x2F;p&gt;
&lt;p&gt;If this is hard to visualize, then fear not, you can browse to thanos &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;thanos.io&#x2F;tip&#x2F;components&#x2F;tools.md&#x2F;#bucket-web&quot;&gt;bucket web&lt;&#x2F;a&gt; to visualise the state of your &lt;code&gt;S3&lt;&#x2F;code&gt; bucket. It&#x27;s a small service (included in the chart) that presents the view, and what resolutions are availble from various dates.&lt;&#x2F;p&gt;
&lt;p&gt;Think of &lt;code&gt;compactor&lt;&#x2F;code&gt; as a cronjob (but with &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;monitoring.mixins.dev&#x2F;thanos&#x2F;#thanos-compact&quot;&gt;good alerts&lt;&#x2F;a&gt;) that needs to do big data operations. If you give it enough space, cpu, and memory it is usually happy. It will use these resources a bit sporadically though - some cycles are clearly visible in memory use:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;compactor-cycles.png&quot;&gt;&lt;img src=&quot;&#x2F;imgs&#x2F;prometheus&#x2F;compactor-cycles.png&quot; alt=&quot;thanos compactor cpu and memory usage&quot; &#x2F;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;part-3-metrics-api-integrations&quot;&gt;Part 3: Metrics API Integrations&lt;&#x2F;h2&gt;
&lt;p&gt;The final components reside in the void outside the two big standard charts and contains the implementors of the various &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;metrics#apis&quot;&gt;metrics apis&lt;&#x2F;a&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;resource metrics&lt;&#x2F;strong&gt; api (cpu&#x2F;memory for pods)&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;custom metrics&lt;&#x2F;strong&gt; api (metrics related to a scalable object)&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;external metrics&lt;&#x2F;strong&gt; api (metrics unrelated to a scalable object)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;These are apis that allow Kubernetes to &lt;code&gt;scale&lt;&#x2F;code&gt; your workloads (with varying degrees of intelligence) through HPAs, but you need something to implement them.&lt;&#x2F;p&gt;
&lt;p&gt;I mention these different underlying apis explicitly because &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes-sigs&#x2F;custom-metrics-apiserver&#x2F;issues&#x2F;70&quot;&gt;currently&lt;&#x2F;a&gt; you can &lt;strong&gt;only&lt;&#x2F;strong&gt; have &lt;strong&gt;one implementor&lt;&#x2F;strong&gt; of each api, and if you have more than one thing that provides custom metrics (like say cloudwatch metrics + prometheus adapter), then you are better served by using &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;keda.sh&#x2F;&quot;&gt;KEDA&lt;&#x2F;a&gt; than what is described herein.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;metrics-server&quot;&gt;metrics-server&lt;&#x2F;h3&gt;
&lt;p&gt;The first is a kubernetes standard component; the &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes-sigs&#x2F;metrics-server&quot;&gt;metrics-server&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;It only implements the &lt;strong&gt;resource metrics&lt;&#x2F;strong&gt; api, and thus &lt;strong&gt;only&lt;&#x2F;strong&gt; enables you scale on cpu and memory.&lt;&#x2F;p&gt;
&lt;p&gt;It extracts cpu&#x2F;memory values values via &lt;code&gt;kubelet&lt;&#x2F;code&gt;, and as such allows &lt;code&gt;kubectl top&lt;&#x2F;code&gt; + HPAs to work out of the box - without prometheus or any of the other components visualised herein. It&#x27;s even installed on &lt;code&gt;k3d&lt;&#x2F;code&gt; by default.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;prometheus-adapter&quot;&gt;prometheus-adapter&lt;&#x2F;h3&gt;
&lt;p&gt;This adapter funnels metrics from &lt;code&gt;prometheus&lt;&#x2F;code&gt; into the HPA universe, so you can scale on &lt;strong&gt;arbitrary&lt;&#x2F;strong&gt; metrics.&lt;&#x2F;p&gt;
&lt;p&gt;It implements the resource metrics, custom metrics, and external metrics APIs. The underlying setup for this has &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;tasks&#x2F;run-application&#x2F;horizontal-pod-autoscale&#x2F;#scaling-on-custom-metrics&quot;&gt;stable docs from k8s 1.23&lt;&#x2F;a&gt;, and in essence this allows you to scale on &lt;strong&gt;custom metrics&lt;&#x2F;strong&gt; (related to the scaling object) or &lt;strong&gt;external metrics&lt;&#x2F;strong&gt; (unrelated to the scaling object).&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes-sigs&#x2F;prometheus-adapter&#x2F;blob&#x2F;c9e69613d3e1ccf4a5828aba25de613d84694779&#x2F;docs&#x2F;sample-config.yaml&quot;&gt;syntax needed for this component&lt;&#x2F;a&gt; definitely leaves a lot &lt;strong&gt;to be desired&lt;&#x2F;strong&gt;. The only way we have managed to get somewhere with this is with principal engineers and a lot of trial and error. Thankfully, there are &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes-sigs&#x2F;prometheus-adapter&#x2F;blob&#x2F;master&#x2F;docs&#x2F;walkthrough.md&quot;&gt;some helpful resources&lt;&#x2F;a&gt;, this is still not easy.&lt;&#x2F;p&gt;
&lt;p&gt;The repository for &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes-sigs&#x2F;prometheus-adapter&quot;&gt;prometheus-adapter&lt;&#x2F;a&gt; is also not receiving a whole of attention: &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;kubernetes-sigs&#x2F;prometheus-adapter&#x2F;issues?q=is%3Aissue+is%3Aclosed+label%3Alifecycle%2Frotten&quot;&gt;almost half&lt;&#x2F;a&gt; of their closed issues looks like they were closed by kubernetes org&#x27;s auto-closer bot. You can say many sensible things about this closing practice - on the importance of funding and triagers for open source software - but it ultimately sends a message:&lt;&#x2F;p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Unpopular opinion: I think solving issue triage by letting robots auto close issues that were never responded to is a *horrible* way to manage your project and tells users you don&amp;#39;t give a crap about their effort filing bugs :(&lt;&#x2F;p&gt;&amp;mdash; Benjamin Elder (@BenTheElder) &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;BenTheElder&#x2F;status&#x2F;1407774856033181696?ref_src=twsrc%5Etfw&quot;&gt;June 23, 2021&lt;&#x2F;a&gt;&lt;&#x2F;blockquote&gt; &lt;script async src=&quot;https:&#x2F;&#x2F;platform.twitter.com&#x2F;widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;&#x2F;script&gt;
&lt;p&gt;It does have its own &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;prometheus-community&#x2F;helm-charts&#x2F;tree&#x2F;main&#x2F;charts&#x2F;prometheus-adapter&quot;&gt;prometheus-community maintained chart&lt;&#x2F;a&gt;, which is of high quality, but you will need to figure out the templated promql yourself.&lt;&#x2F;p&gt;
&lt;p&gt;Without having much experience with &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;keda.sh&#x2F;&quot;&gt;KEDA&lt;&#x2F;a&gt;, I would recommend looking into using &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;keda.sh&#x2F;docs&#x2F;2.5&#x2F;scalers&#x2F;prometheus&#x2F;&quot;&gt;KEDA&#x27;s prometheus scaler directly instead&lt;&#x2F;a&gt; of using the arcane template magic from &lt;code&gt;prometheus-adapter&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;happy-new-year&quot;&gt;Happy new year&lt;&#x2F;h2&gt;
&lt;p&gt;That&#x27;s all I can bring myself to write about this archeticture for now. It took longer than I anticipated, so hopefully this was useful to someone. Regardless, best of luck maintaining prometheus in 2022.&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
